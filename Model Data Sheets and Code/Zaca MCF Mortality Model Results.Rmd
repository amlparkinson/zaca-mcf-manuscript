
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,
                      warning = F,
                      message = F)
```

### Load data and packages
```{r}
library(randomForest)
library(pdp)
library(caret)
library(tidyverse)
library(ggpubr)
library(caret)
library(stringr)
library(here)
library(janitor)
library(ggfortify)
library(ALEPlot)
library(iml)
library(ROCR)
library(pROC)

mcf <- read.csv(here::here("Model Data Sheets and Code", 'Zaca MCF variables for model.csv')) %>% 
  mutate(mortality_qual= fct_relevel(mortality_qual, levels=c("High", "Moderate", "Low", "None"))) %>% 
  mutate(dnbr_qualitative = case_when(
    dnbr_2007 < 0.1 ~ "None",
    dnbr_2007 < 0.27 ~ "Low",
    dnbr_2007 < 0.66 ~ "Moderate",
    dnbr_2007 > 0.66 ~ "High")) %>%
  mutate(dnbr_qualitative = as.factor(dnbr_qualitative),
         dnbr_qualitative = fct_relevel(dnbr_qualitative, levels=c("High", "Moderate", "Low", "None"))) %>%
  mutate(aspect_transformed = (1-cos((2*pi*aspect)/360))/2) %>% 
  mutate(three_class_mortality = case_when(
    mortality_qual %in% "High" ~ "High",
    mortality_qual %in% "Moderate" ~ "Moderate",
    mortality_qual %in% c("Low", "None") ~ "Low")) %>% 
  mutate(three_class_mortality = as.factor(three_class_mortality)) %>% 
  mutate(three_class_mortality= fct_relevel(three_class_mortality, 
                                     levels=c("Low", "Moderate", "High"))) %>% 
  mutate(two_class_mortality = case_when(
    mortality_qual %in% "High" ~ "High",
    mortality_qual %in% c("Moderate", "Low", "None") ~ "Other")) %>% 
  mutate(two_class_mortality = as.factor(two_class_mortality)) %>% 
  mutate(two_class_mortality= fct_relevel(two_class_mortality, 
                                     levels=c("Other", "High"))) %>% 
  mutate(tc_mortality = as.factor(tc_mortality)) %>% 
  mutate(rdnbr_qualitative = case_when(
    rdnbr_2007 < 69 ~ "None",
    rdnbr_2007 < 315 ~ "Low",
    rdnbr_2007 < 640 ~ "Moderate",
    rdnbr_2007 > 640 ~ "High")) %>%
  mutate(rdnbr_qualitative = as.factor(rdnbr_qualitative),
         rdnbr_qualitative = fct_relevel(rdnbr_qualitative, 
                                         levels=c("High", "Moderate", "Low", "None"))) %>% 
    mutate(rdnbr_qualitative2 = case_when(
    rdnbr_qualitative %in% "High" ~ "High",
    rdnbr_qualitative %in% c("Moderate", "Low", "None") ~ "Other")) %>% 
  mutate(rdnbr_qualitative2 = as.factor(rdnbr_qualitative2)) %>% 
  mutate(rdnbr_qualitative2= fct_relevel(rdnbr_qualitative2, 
                                     levels=c("Other", "High")))  %>% 
  mutate(rdnbr_tc = case_when(
    rdnbr_2007 < 1000 ~ "Not_TC",
    rdnbr_2007 >= 1000 ~ "TC"
  )) %>% 
  mutate(rdnbr_tc = as.factor(rdnbr_tc)) %>% 
  mutate(rdnbr_tc= fct_relevel(rdnbr_tc, 
                                     levels=c("Not_TC", "TC")))
  



 #mutate(mortality_qual= fct_relevel(mortality_qual, levels=c("None", "Low", "Moderate", "High")))
 # keep getting error (Outer names are only allowed for unnamed scalar atomic inputs) when order the levels, but the reordering works anyway (although it doesnt work with the ssame error in the binomial code. The code directly below works to reorder the levels without error)

pico <- read_csv("coulter_env_modified.csv") %>% 
  mutate(rdnbr_qualitative = case_when(
    rdnbr_2007 < 69 ~ "None",
    rdnbr_2007 < 315 ~ "Low",
    rdnbr_2007 < 640 ~ "Moderate",
    rdnbr_2007 > 640 ~ "High")) %>%
  mutate(rdnbr_qualitative = as.factor(rdnbr_qualitative),
         rdnbr_qualitative = fct_relevel(rdnbr_qualitative, 
                                         levels=c("High", "Moderate", "Low", "None"))) %>%
  dplyr::select(-Mortality, -type_converted, -ndvi_2006) %>% 
  na.omit(pico) %>%  # 56 stands have NA values (probs all very small stands)
  #filter(Mortality != "bigcone") %>% # 60 stands actually have big cones not pico
  mutate(rdnbr_qualitative2 = case_when(
    rdnbr_qualitative %in% "High" ~ "High",
    rdnbr_qualitative %in% c("Moderate", "Low", "None") ~ "Other")) %>% 
  mutate(rdnbr_qualitative2 = as.factor(rdnbr_qualitative2)) %>% 
  mutate(rdnbr_qualitative2= fct_relevel(rdnbr_qualitative2, 
                                     levels=c("Other", "High"))) %>% 
  mutate(aspect_transformed = (1-cos((2*pi*aspect_30m)/360))/2) %>% 
  rename_all(funs(str_replace(., "_30m", ""))) %>% 
  mutate(dnbr_qualitative = case_when(
    dnbr_2007 < 0.1 ~ "None",
    dnbr_2007 < 0.27 ~ "Low",
    dnbr_2007 < 0.66 ~ "Moderate",
    dnbr_2007 > 0.66 ~ "High")) %>%
  mutate(dnbr_qualitative = as.factor(dnbr_qualitative),
         dnbr_qualitative = fct_relevel(dnbr_qualitative, levels=c("High", "Moderate", "Low", "None")))  
  
  

  #document name is "modified" bc i manually removed all the unnesessary arc columns

```
for dnbr classes see: https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLI-B8/763/2016/isprs-archives-XLI-B8-763-2016.pdf

for aspect transformation: https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/07-0539.1 appendix B; transformed Aspect = [1 - cos(2piAspect/360)]/2. This formula is similar (but not identical) to that used by Roberts and Cooper (1989). The transformed aspect values lie in the interval from 0 to1. Values near 0 represent aspects close to due north, while values near 1 represent aspects close to due south. East and west are treated identically with transformed aspect values of 0.5.

Need to remove the bigcone stands from the pico dataset but having a little trouble atm. 

### Methods background

using forced sample sizes (see https://stats.stackexchange.com/questions/457451/how-r-randomforest-sampsize-works) because the distribution of observations in each mortality class is very uneven and results in a model that severely underpredicts observations in the mortality class with few observations. Bc of the small sample size, we will not be using training and test data (and bc RF basically creates its own train/test data).

Different RF trials:

  - Mixed confier forests
    + response variable: type conversion
    + response variable: 4 mortality classes (none, low, momderate, high)
    + response variable: 3 mortality classes (none/low, moderate, high)
    + response variable: 2 mortality classes (none/low/moderate, high)
    + response variable: rdnbr (regression)
  - Coulter pines 
    + response variable: rdnbr (regression)
  - mixed conifer forests and coulter pines combined
    + response variable: rdnbr (regression)

Note: manual classification of mortality and type conversion for the coulter pines was not done. Aint no one got time for that

The type conversion mortality classes-
tc_mortality 0 = "not_tc" (n=99)
tc_mortality 1 = type converted (n=>600)

### Goals/Questions

1. Can the mcf model distinguish between type conversion and the other mortality types? 
    yes, if we use ALE plots
    
2. Which variables contribute to type conversion of MCFs and pico?

3. How well does rdnbr compare to manual estimates of mortality? 
    high rdnbr matches up very well with high mortality. There is less of a match up between moderate, low,      and no mortality with the corresponding rdnbr severity levels. 

4. How does using rdnbr and manually derived mortality compare as a response variable?
    this would be a better question for the big cone model

5. Which variables contribute to high/low/type conversion rdnbr? Is there a distinguishable difference between values of the explanatory variables for high/type conversion levels of rdnbr?

6. Does conifer type contribute to different variable importances?


### Sample Size for each mortality class
```{r}
#mcf -------------------------------------------------------------------
mortality_count <- mcf %>% 
  group_by(mortality_qual) %>% 
  count() %>% 
  #rename(manual_mortality=mortality_qual) %>% 
  mutate(mortality_type = 'manual')

rdnbr_mcf <- mcf %>% 
  group_by(rdnbr_qualitative) %>% 
  count() %>% 
  rename(mortality_qual = rdnbr_qualitative) %>% 
  mutate(mortality_type = 'rdnbr')

dnbr_mcf <- mcf %>% 
  group_by(dnbr_qualitative) %>% 
  count() %>% 
  rename(mortality_qual = dnbr_qualitative) %>% 
  mutate(mortality_type = 'dnbr')

mcf_sample_size <- rbind(mortality_count, rdnbr_mcf, dnbr_mcf)

ggplot(mcf_sample_size, aes(x = mortality_qual, y = n, fill = mortality_type)) + 
  geom_col(position = "dodge") +
  geom_text(aes(label=n), position = position_dodge(width=0.9), vjust=-0.5) +
  scale_fill_discrete(name = "How was mortality determined")

# mcf manual type conversison ------------------------------------------------------------------- 
tc_count <- mcf %>% 
  group_by(tc_mortality) %>% 
  count() # 99 stands with type conversion, 636 with no type conversion

# mcf type conversion with rdnbr > 1000 threshold --------------------------------------------
tc2_coun <- mcf %>% 
  group_by(rdnbr_tc) %>% 
  count() #not tc = 680, tc = 55

#coulter ------------------------------------------------------------------- 
rdnbr_pico <- pico %>% 
  group_by(rdnbr_qualitative) %>% 
  count() %>% 
  rename(mortality_qual =rdnbr_qualitative) %>% 
  mutate(mortality_type = 'rdnbr')

dnbr_pico <- pico %>% 
  group_by(dnbr_qualitative) %>% 
  count() %>% 
  rename(mortality_qual = dnbr_qualitative)%>% 
  mutate(mortality_type = 'dnbr')

pico_sample_size <- rbind(rdnbr_pico, dnbr_pico)

ggplot(pico_sample_size, aes(x = mortality_qual, y = n, fill = mortality_type)) + 
  geom_col(position = "dodge") +
  geom_text(aes(label=n), position = position_dodge(width=0.9), vjust=-0.5) +
  scale_fill_discrete(name = "How was mortality determined")

```
MCF: Yah, dnbr is not a good index to use. Rdnbr is better at least when it comes to high severity and high mortality. Both dnbr and rdnbr over estimate moderate adn low mortlity and severely underestiamte stands with no mortality. Bc of this, we could make the assumption that pico stands with high rdnbr also have high mortality. 

## Correlation

```{r}
mcf_corr <- mcf %>% 
  select(dem_10m_zaca:TPI_five)
correlation <- cor(mcf_corr$dem_10m_zaca, mcf_corr$TPI_five) # this works but can only compare 2 columns at a time
cor(mcf_corr[dem_10m_zaca:TPI_five]) # wont work :/ 
corr.test(mcf_corr[dem_10m_zaca:TPI_five]) # neither does this; library(psych)
sapply(mcf_corr, cor, y=mcf_corr$sr) # kinda works but only compares correlatoins to one column at atime
with(mcf_corr, cor(mcf_corr[dem_10m_zaca:TPI_five]))
```
idk why this aint workin man. I cleaned the environment and used the minimum number of packages/read_csv()  code necessary. its worked before! 

### Stepwise Variable Selection

```{r}

mcf_sub <- mcf %>% 
  dplyr::select(rdnbr_qualitative,flow_accumulation, sr, rain, slope, tmax, TPI_five, TWI, fri, aspect_transformed, cwd, ndvi, vpd_aug, vpd_jan,ndmi_2006, dem_10m_zaca)

#Extract prediction and response variables
mcf.predictors <- mcf_sub[,2:ncol(mcf_sub)]
mcf.response <- mcf_sub[,1]

#Run stepwise variable selection algorithm
mcf.vsurf <- VSURF(x = mcf.predictors, y = mcf.response)


#Get names of selected variables
names(mcf.predictors[mcf.vsurf$varselect.pred])



# does vsurf selected variables improve variance explained and result in stabilized variable importance ranking?

for (i in 1:7) {
  
rfm <- randomForest(rdnbr_tc ~ rain + tmax + flow_accumulation + sr + slope + TWI + cwd + vpd_aug + ndvi + ndmi_2006 + aspect_transformed + vpd_jan, 
                    data = mcf, 
                    sampsize = c('Not_TC' = 55, 'TC' = 55), 
                    ntree=1500, 
                    importance=T,
                    replace = T)
# OOB and importance
rfm
importance(rfm)
#plot to visualize variable importance
varImpPlot(rfm)

}
rfm_roc <-  roc(mcf$rdnbr_tc, rfm$votes[,1])
auc(rfm_roc)

```
all variables: rain + tmax + TPI_five + dem_10m_zaca + flow_accumulation + sr + slope + TWI + cwd + vpd_aug + ndvi + ndmi_2006 + vpd_jan + aspect_transformed

vsurf variable selection (mcf, regression): slope + sr + vpd_aug + tmax + ndmi_2006 + vpd_jan + ndvi + rain + dem_10m_zaca

vsurf variables: % variance explained = 50%, VI ranking is NOT stable, msr = 44,577
all variables: % variance explained = 47%, VI ranking is NOT stable, msr = 47,592
all variables (minus elevation): % variance explained = 47%, VI ranking is mostly stable
all variables (minus vpd_jan and elevation): % variance explained = 44%, VI ranking IS very stable
all variables (minus aspect, tpi, twi): % variance explained = 50%, VI ranking is not that stable, msr = 44,699
all variables (minus aspect, tpi, twi, vpd_jan): % variance explained = 49%, VI ranking is kinda stable
all variables (minus aspect, tpi, twi, elevation): % variance explained = 49%, VI ranking is kinda stable


vsurf variable selection (mcf, classification, rdnr_tc): vpd_aug + slope + rain + vpd_jan + ndvi

vsurf variables: oob = 14%, not tc class error = 13%, tc class error = 25%, VI ranking IS stable, auc = 89%
all variables: oob = 15%, not tc class error = 14%, tc class error = 31%, VI ranking is NOT stable, auc = 86%
all variables (minus elevation, tpi, and vpd_jan): oob = 15%, not tc class error = 14%, tc class error = 35%, VI ranking IS stable, auc = 84%
all variables(minus elevation, tpi): oob = 15%, not tc class error = 13%, tc class error = 31%, VI ranking is not stable, auc = 86%

vsurf variable selection (mcf, classification, rdnbr 4 mortality classes): sr + ndmi_2006 + vdp_aug + dem_10m_zaca + ndvi + vpd_jan + tmax + cwd + rain + flow_accumulation


### Stepwise Variable Selection - PICO

```{r}

pico_sub <- pico %>% 
  dplyr::select(flow_accumulation, insolation, rain, slope, tmax, TPI_five, TWI,  aspect_transformed, cwd,  vpd_aug, vpd_jan,ndmi_2006, dem, years_since_burn) # no ndvi

#Extract prediction and response variables
pico.predictors <- pico_sub[,c(2:14)]
pico.response <- pico_sub[, 1]

#Run stepwise variable selection algorithm
pico.vsurf <- VSURF(pico$rdnbr_2007 ~ ., data = pico_sub) #setting up the code the same way as the mcf resulted in an error, but this format also works


#Get names of selected variables
names(pico_sub[pico.vsurf$varselect.pred])



# does vsurf selected variables improve variance explained and result in stabilized variable importance ranking?

for (i in 1:7) {
  
rfm <- randomForest(rdnbr_2007 ~ flow_accumulation + insolation + + rain + ndmi_2006 + tmax + dem + vpd_jan + vpd_aug + cwd + years_since_burn, 
                    data = pico, 
                    #sampsize = c('Not_TC' = 55, 'TC' = 55), 
                    ntree=1500, 
                    importance=T,
                    replace = T)
# OOB and importance
rfm
importance(rfm)
#plot to visualize variable importance
varImpPlot(rfm)

}
rfm_roc <-  roc(mcf$rdnbr_tc, rfm$votes[,1])
auc(rfm_roc)

```
VSURF regreession variables selected: flow accumulation + insolation + + rain + ndmi_2006 + tmax + dem + vpd_jan + vpd_aug + cwd + years_since_burn 


## type conversion threshold for rdnbr
```{r}
##plot -------------------------------------------------------------------------------

# mcf
ggplot(mcf, aes(x = tc_mortality, y = rdnbr_2007)) +
  geom_jitter()

##by prefire cover
ggplot(mcf, aes(x = tc_mortality, y = rdnbr_2007)) +
  geom_jitter() +
  facet_wrap(~prefire)


## use lm to get a rdnbr threshold for the type converter stands? pretty sure thats not what you meant chris ---------------------

threshold <- lm(rdnbr_2007 ~ tc_mortality, data = mcf)
summary(threshold) #r2=0.53

threshold1 <- lm(rdnbr_2007 ~ tc_mortality + ndvi, data = mcf)
summary(threshold1) #r2=0.54

threshold2 <- lm(rdnbr_2007 ~ tc_mortality + prefire, data = mcf)
summary(threshold2) #r2=0.54

# max type converted values ----------------------------------------------

max_nonTC <- mcf %>% 
  group_by(tc_mortality) %>% 
  summarise(max = max(rdnbr_2007),
            min = min(rdnbr_2007)) 
  #non-type converted=1160.433 ; type converted=1414.205

# potential type conversion threshold --------------------------------------------
## pretty sure i did NOT followed the correct steps to pick this threshold. 
tc_threshold <- mcf %>% 
  mutate(rdnbr_tc_threshold = case_when(
    rdnbr_2007 <1160 ~ "not_tc",
    rdnbr_2007 >1160 ~ "tc"
  )) %>% 
  mutate(rdnbr_tc_threshold = as.factor(rdnbr_tc_threshold))

tc_threshold_count <- tc_threshold %>% 
  group_by(rdnbr_tc_threshold) %>% 
  count() # not tc = 702, tc = 33

```
well there some overlap with the the 2 classes, but there is also a clear point that the non type converted stands reach (rdnbr=1160) that many type converted stands surpass. 

##high mortality v rdnbr
```{r}
rdnbr_qual_n <- mcf %>% 
  group_by(rdnbr_qualitative2) %>% 
  count() %>% 
  mutate(mort_type = "rdnbr") %>% 
  rename(two_class_mortality = rdnbr_qualitative2)

mortality_n <- mcf %>% 
  group_by(two_class_mortality) %>% 
  count() %>% 
  mutate(mort_type = "manual")

mortality_count <- rbind(rdnbr_qual_n, mortality_n)

ggplot(mortality_count, aes(x = two_class_mortality, y = n, fill = mort_type)) + geom_col(position = "dodge") +
  geom_text(aes(label=n), position = position_dodge(width=0.9), vjust=-0.5) +
  scale_fill_discrete(name = "How was mortality determined")

```


### Data Spread

```{r}
solar_rad_mcf <- ggplot(mcf, aes(x=mortality_qual, y=sr, fill=mortality_qual))+
  geom_violin(alpha=0.5, show.legend=F) +
  theme_classic() +
  labs(x="Mortality", y="Solar Radiation") +
  scale_fill_manual(values=c("green", "yellow", "orange", "red")) +
  coord_flip()

tmax_mcf <- ggplot(mcf, aes(x=mortality_qual, y=tmax, fill=mortality_qual))+
  geom_violin(alpha=0.5, show.legend=F) +
  theme_classic() +
  labs(x="Mortality", y="Max Aug Temp (celcius)") +
  scale_fill_manual(values=c("green", "yellow", "orange", "red")) +
  coord_flip()

rain_mcf <- ggplot(mcf, aes(x=mortality_qual, y=rain, fill=mortality_qual))+
  geom_violin(alpha=0.5, show.legend=F) +
  theme_classic() +
  labs(x="Mortality", y="Precipitation") +
  scale_fill_manual(values=c("green", "yellow", "orange", "red")) +
  coord_flip()

tpi_mcf <- ggplot(mcf, aes(x=mortality_qual, y=TPI_five, fill=mortality_qual))+
  geom_violin(alpha=0.5, show.legend=F) +
  theme_classic() +
  labs(x="Mortality", y="TPI") +
  scale_fill_manual(values=c("green", "yellow", "orange", "red")) +
  coord_flip()

twi_mcf <- ggplot(mcf, aes(x=mortality_qual, y=TWI, fill=mortality_qual))+
  geom_violin(alpha=0.5, show.legend=F) +
  theme_classic() +
  labs(x="Mortality", y="TWI") +
  scale_fill_manual(values=c("green", "yellow", "orange", "red")) +
  coord_flip()

slope_mcf <- ggplot(mcf, aes(x=mortality_qual, y=slope, fill=mortality_qual))+
  geom_violin(alpha=0.5, show.legend=F) +
  theme_classic() +
  labs(x="Mortality", y="Slope") +
  scale_fill_manual(values=c("green", "yellow", "orange", "red")) +
  coord_flip()

elev_mcf <- ggplot(mcf, aes(x=mortality_qual, y=dem_10m_zaca, fill=mortality_qual))+
  geom_violin(alpha=0.5, show.legend=F) +
  theme_classic() +
  labs(x="Mortality", y="Elevation") +
  scale_fill_manual(values=c("green", "yellow", "orange", "red")) +
  coord_flip()

cwd_mcf <- ggplot(mcf, aes(x=mortality_qual, y=cwd, fill=mortality_qual))+
  geom_violin(alpha=0.5, show.legend=F) +
  theme_classic() +
  labs(x="Mortality", y="CWD") +
  scale_fill_manual(values=c("green", "yellow", "orange", "red")) +
  coord_flip()

aug_vpd_mcf <- ggplot(mcf, aes(x=mortality_qual, y=vpd_aug, fill=mortality_qual))+
  geom_violin(alpha=0.5, show.legend=F) +
  theme_classic() +
  labs(x="Mortality", y="Aug VPD") +
  scale_fill_manual(values=c("green", "yellow", "orange", "red")) +
  coord_flip()

jan_vpd_mcf <- ggplot(mcf, aes(x=mortality_qual, y=vpd_jan, fill=mortality_qual))+
  geom_violin(alpha=0.5, show.legend=F) +
  theme_classic() +
  labs(x="Mortality", y="Jan VPD") +
  scale_fill_manual(values=c("green", "yellow", "orange", "red")) +
  coord_flip()

ggplot(mcf, aes(x=mortality_qual, y=ndvi, fill=mortality_qual))+
  geom_violin(alpha=0.5, show.legend=F) +
  theme_classic() +
  labs(x="Mortality", y="NDVI") +
  scale_fill_manual(values=c("green", "yellow", "orange", "red")) +
  coord_flip()

ggplot(mcf, aes(x=ndvi, y=rdnbr_2007)) +
  geom_point()

ndvi_rdnbr_corr <- lm(rdnbr_2007~ndvi, data=mcf)
summary(ndvi_rdnbr_corr) #3%
ndmi_rdnbr_corr <- lm(rdnbr_2007~ndmi_2007, data=mcf)
summary(ndmi_rdnbr_corr) #82.7%

ggarrange(solar_rad_mcf, rain_mcf, slope_mcf, elev_mcf, tmax_mcf, tpi_mcf, twi_mcf, cwd_mcf, aug_vpd_mcf, jan_vpd_mcf)
#ggsave("zaca MCF most important variable violin plots.png")
```

Most distinction between classes in slope, elevation, and solar radiation. Could explain why those variables tend to have the highest variable importance. 
Interestingly, stands with higher elevation had higher mortality and stands with steeper slope and higher max temp had lower/no mortality. Both of which are counter-intuitive. 

### PCA

```{r}
mcf_sub <- mcf %>% 
  select (dem_10m_zaca:sr, rain:tmax, TPI_five, TWI, cwd:aspect_transformed) %>% 
  rename ('Elevation' = 'dem_10m_zaca',
          'Slope' = 'slope',
          'TPI' = 'TPI_five',
          'Precip' = 'rain',
          'Solar Radiation' = 'sr',
          'Flow Accumulation' = 'flow_accumulation',
          'CWD' = 'cwd',
          'Temperature' = 'tmax',
          'VPD (Jan)' = 'vpd_jan',
          'VPD (Aug)' = 'vpd_aug')

mcf_sub_pca <- prcomp(mcf_sub, scale = T)

#summary
summary(mcf_sub_pca)
mcf_sub_pca

#plot
mcf_sub_pca_plot <- autoplot(mcf_sub_pca, 
                          colour = NA, # HAVE to spell it color this way (colour) bc thats the only spelling form autoplot recognizes
                          loadings.label = T,
                          loadings.label.size = 3,
                          loadings.label.colour = "black",
                          loadings.label.repel = T) +
  scale_y_continuous(lim=c(-0.15, 0.15)) +
  labs(title = "Zaca Fire MCF PCA") +
  theme_minimal()

mcf_sub_pca_plot

```

### RF manual mortality: 4 classes (none, low, moderate, high)
```{r}
for (i in 1:10) {
  
rfm <- randomForest(mortality_qual ~ rain + tmax + TPI_five + dem_10m_zaca + flow_accumulation + sr + slope + TWI + cwd + vpd_aug + vpd_jan + aspect_transformed + ndvi + ndmi_2006 + rdnbr_qualitative, 
                    data = mcf, 
                    sampsize = c('None' = 158, 'Low' = 158, 'Moderate' = 158, 'High' = 158), 
                    ntree=1500, 
                    importance=T,
                    replace = T)
# OOB and importance
rfm
importance(rfm)
#plot to visualize variable importance
varImpPlot(rfm)

plot_fi(rf_feat_importance(rfm, mcf))

}
```

PDP
```{r}
#PDP
a <- partialPlot(rfm, as.data.frame(mcf), vpd_jan, "None") 
c <- partialPlot(rfm, as.data.frame(mcf), vpd_jan, "Low") 
d <- partialPlot(rfm, as.data.frame(mcf), vpd_jan, "Moderate") 
b <- partialPlot(rfm, as.data.frame(mcf), vpd_jan, "High", col="red", ylim=c(-0.8,0.8)) 
lines(a, c, d, col= "green", "yellow", "orange") # eh, not working. forgot how to set this up 2 show all 4 off the top of my head

a <- partialPlot(rfm, as.data.frame(mcf), slope, "None") 
c <- partialPlot(rfm, as.data.frame(mcf), slope, "Low") 
d <- partialPlot(rfm, as.data.frame(mcf), slope, "Moderate") 
b <- partialPlot(rfm, as.data.frame(mcf), slope, "High", col="red", ylim=c(-0.8,0.8)) 
lines(a, col="green")

a <- partialPlot(rfm, as.data.frame(mcf), sr, "None") 
c <- partialPlot(rfm, as.data.frame(mcf), sr, "Low") 
d <- partialPlot(rfm, as.data.frame(mcf), sr, "Moderate")  
b <- partialPlot(rfm, as.data.frame(mcf), sr, "High", col="red") 
lines(a, col="green")


```

ALE
```{r}
#sub data
mcf_ale <- mcf %>% 
  select(mortality_qual, rain, tmax, TPI_five, dem_10m_zaca, flow_accumulation, sr, slope, TWI, cwd, vpd_aug, vpd_jan, ndmi_2006, aspect_transformed, ndvi, rdnbr_qualitative) # the predictor$new code had issues with using a df with NA values even thought the variabels in the RF do not have NA values. 

train_x <- mcf_ale[-which(names(mcf_ale) == "mortality_qual")]

# predictors 
none_predictor <- Predictor$new(rfm, data = mcf_ale, y = mcf_ale$mortality_qual, type = "prob", class="None")
low_predictor <- Predictor$new(rfm, data = mcf_ale, y = mcf_ale$mortality_qual, type = "prob", class="Low")
moderate_predictor <- Predictor$new(rfm, data = mcf_ale, y = mcf_ale$mortality_qual, type = "prob", class="Moderate")
high_predictor <- Predictor$new(rfm, data = mcf_ale, y = mcf_ale$mortality_qual, type = "prob", class="High")

#vpd_jan
none <- plot(FeatureEffect$new(none_predictor, feature = "vpd_jan")) + ggtitle("None")
low <- plot(FeatureEffect$new(low_predictor, feature = "vpd_jan")) + ggtitle("Low")
moderate <- plot(FeatureEffect$new(moderate_predictor, feature = "vpd_jan")) + ggtitle("Moderate")
high <- plot(FeatureEffect$new(high_predictor, feature = "vpd_jan")) + ggtitle("High")
ggarrange(none, low, moderate, high)

#slope
none <- plot(FeatureEffect$new(none_predictor, feature = "slope")) + ggtitle("None")
low <- plot(FeatureEffect$new(low_predictor, feature = "slope")) + ggtitle("Low")
moderate <- plot(FeatureEffect$new(moderate_predictor, feature = "slope")) + ggtitle("Moderate")
high <- plot(FeatureEffect$new(high_predictor, feature = "slope")) + ggtitle("High")
ggarrange(none, low, moderate, high)

#sr
none <- plot(FeatureEffect$new(none_predictor, feature = "sr")) + ggtitle("None")
low <- plot(FeatureEffect$new(low_predictor, feature = "sr")) + ggtitle("Low")
moderate <- plot(FeatureEffect$new(moderate_predictor, feature = "sr")) + ggtitle("Moderate")
high <- plot(FeatureEffect$new(high_predictor, feature = "sr")) + ggtitle("High")
ggarrange(none, low, moderate, high)


```



### RF manual mortality: 3 classes (none/low, moderate, high)
```{r}
for (i in 1:10) {
  
rfm <- randomForest(three_class_mortality ~ rain + tmax + TPI_five + dem_10m_zaca + flow_accumulation + sr + slope + TWI + cwd + vpd_aug + vpd_jan + aspect_transformed, 
                    data = mcf, 
                    sampsize = c('Low' = 158, 'Moderate' = 158, 'High' = 158),
                    ntree=1500, 
                    importance=T,
                    replace = T)
# OOB and importance
rfm
importance(rfm)
#plot to visualize variable importance
varImpPlot(rfm)

}
```

PDP
```{r}
#PDP
c <- partialPlot(rfm, as.data.frame(mcf), vpd_jan, "Low") 
d <- partialPlot(rfm, as.data.frame(mcf), vpd_jan, "Moderate") 
b <- partialPlot(rfm, as.data.frame(mcf), vpd_jan, "High", col="red", ylim=c(-0.8,0.8)) 

c <- partialPlot(rfm, as.data.frame(mcf), slope, "Low") 
d <- partialPlot(rfm, as.data.frame(mcf), slope, "Moderate") 
b <- partialPlot(rfm, as.data.frame(mcf), slope, "High", col="red", ylim=c(-0.8,0.8))
lines(c, col="green")


c <- partialPlot(rfm, as.data.frame(mcf), sr, "Low") 
d <- partialPlot(rfm, as.data.frame(mcf), sr, "Moderate")  
b <- partialPlot(rfm, as.data.frame(mcf), sr, "High", col="red") 

```

ALE
```{r}
#sub data
mcf_ale <- mcf %>% 
  select(three_class_mortality, rain, tmax, TPI_five, dem_10m_zaca, flow_accumulation, sr, slope, TWI, cwd, vpd_aug, vpd_jan) # the predictor$new code had issues with using a df with NA values even thought the variabels in the RF do not have NA values. 

train_x <- mcf_ale[-which(names(mcf_ale) == "three_class_mortality")]

# predictors 
low_predictor <- Predictor$new(rfm, data = mcf_ale, y = mcf_ale$three_class_mortality, type = "prob", class="Low")
moderate_predictor <- Predictor$new(rfm, data = mcf_ale, y = mcf_ale$three_class_mortality, type = "prob", class="Moderate")
high_predictor <- Predictor$new(rfm, data = mcf_ale, y = mcf_ale$three_class_mortality, type = "prob", class="High")

#vpd_jan
low <- plot(FeatureEffect$new(low_predictor, feature = "vpd_jan")) + ggtitle("Low")
moderate <- plot(FeatureEffect$new(moderate_predictor, feature = "vpd_jan")) + ggtitle("Moderate")
high <- plot(FeatureEffect$new(high_predictor, feature = "vpd_jan")) + ggtitle("High")
ggarrange(low, moderate, high)

#slope
low <- plot(FeatureEffect$new(low_predictor, feature = "slope")) + ggtitle("Low")
moderate <- plot(FeatureEffect$new(moderate_predictor, feature = "slope")) + ggtitle("Moderate")
high <- plot(FeatureEffect$new(high_predictor, feature = "slope")) + ggtitle("High")
ggarrange(low, moderate, high)

#sr
low <- plot(FeatureEffect$new(low_predictor, feature = "sr")) + ggtitle("Low")
moderate <- plot(FeatureEffect$new(moderate_predictor, feature = "sr")) + ggtitle("Moderate")
high <- plot(FeatureEffect$new(high_predictor, feature = "sr")) + ggtitle("High")
ggarrange(low, moderate, high)

```

### RF manual mortality: 2 classes (none/low/moderate (ie other), high)
```{r}
for (i in 1:10) {
  
rfm <- randomForest(two_class_mortality ~ rain + tmax + TPI_five + dem_10m_zaca + flow_accumulation + sr + slope + TWI + cwd + vpd_aug + vpd_jan + aspect_transformed, 
                    data = mcf, 
                    sampsize = c('Other' = 158,'High' = 158),
                    ntree=1500, 
                    importance=T,
                    replace = T)
# OOB and importance
rfm
importance(rfm)
#plot to visualize variable importance
varImpPlot(rfm)

}
```
ROC and AUC
```{r}
# code: https://stats.stackexchange.com/questions/188616/how-can-we-calculate-roc-auc-for-classification-algorithm-such-as-random-forest
rfm_roc <-  roc(mcf$two_class_mortality, rfm$votes[,1])
plot(rfm_roc) # plot comes out a little weird
auc(rfm_roc)
```
PDP
```{r}
#PDP
c <- partialPlot(rfm, as.data.frame(mcf), vpd_jan, "Other") 
b <- partialPlot(rfm, as.data.frame(mcf), vpd_jan, "High", col="red", ylim=c(-0.8,0.8)) 
lines(c, col="green")

c <- partialPlot(rfm, as.data.frame(mcf), slope, "Other") 
b <- partialPlot(rfm, as.data.frame(mcf), slope, "High", col="red", ylim=c(-0.8,0.8)) 
lines(c, col="green")

c <- partialPlot(rfm, as.data.frame(mcf), sr, "Other") 
b <- partialPlot(rfm, as.data.frame(mcf), sr, "High", col="red") 

```

### RF manual mortality: 2 classes (none/low (ie other), high)
removed moderate class

```{r}

mcf_2_class_mod <- mcf %>% 
  filter(mortality_qual != "Moderate") %>% 
  mutate(two_class_mortality_mod = case_when(
    mortality_qual %in% c("None", "Low") ~ "Other",
    mortality_qual %in% c("High") ~ "High"
  ))
mcf_2_class_mod$two_class_mortality_mod <- factor(mcf_2_class_mod$two_class_mortality_mod)  # RF still recognizes that there were 4 mortality classes even though filtered one of them out. This allows rf to not recognize the missing class. 

for (i in 1:10) {
  
rfm <- randomForest(two_class_mortality_mod ~ rain + tmax + TPI_five + flow_accumulation + sr + slope + TWI  + aspect_transformed + ndvi + ndmi_2006, 
                    data = mcf_2_class_mod, 
                    sampsize = c('Other' = 158,'High' = 158),
                    ntree=1500, 
                    importance=T,
                    replace = T)
# OOB and importance
rfm
importance(rfm)
#plot to visualize variable importance
varImpPlot(rfm)

}
```
removed vpd_aug and cwd (rain had greater importance than precip and vpd_jan had greater importance than vpd_aug). .Variable importance got worse, so removed vpd_jan bc tmax became more important more often. NOW the variable iportance is a lot more stable! and lo and behold tmax is more important than slope. removed elevation and replaced it with vpd_jan and variabe importance got cray cray again. Current variables result in very stable importance values!

ROC/AUC
```{r}
# code: https://stats.stackexchange.com/questions/188616/how-can-we-calculate-roc-auc-for-classification-algorithm-such-as-random-forest
rfm_roc <-  roc(mcf_2_class_mod$two_class_mortality_mod, rfm$votes[,1])
plot(rfm_roc) # plot comes out a little weird
auc(rfm_roc)
```

PDP
```{r}
#PDP
a <- partialPlot(rfm, as.data.frame(mcf_2_class_mod), vpd_jan, "Other") 
b <- partialPlot(rfm, as.data.frame(mcf_2_class_mod), vpd_jan, "High", col="red", ylim=c(-0.8,0.8)) 
lines(a, col="green")

a <- partialPlot(rfm, as.data.frame(mcf_2_class_mod), slope, "Other") 
b <- partialPlot(rfm, as.data.frame(mcf_2_class_mod), slope, "High", col="red", ylim=c(-0.8,0.8)) 
lines(a, col="green")

a <- partialPlot(rfm, as.data.frame(mcf_2_class_mod), sr, "Other") 
b <- partialPlot(rfm, as.data.frame(mcf_2_class_mod), sr, "High", col="red") 
lines(a, col="green")

a <- partialPlot(rfm, as.data.frame(mcf_2_class_mod), TPI_five, "Other") 
b <- partialPlot(rfm, as.data.frame(mcf_2_class_mod), TPI_five, "High", col="red", ylim=c(-0.8,0.8)) 
lines(a, col="green")

```

ALE
```{r}
#sub data
mcf_ale <- mcf_2_class_mod %>% 
  select(two_class_mortality_mod, rain, tmax, TPI_five, dem_10m_zaca, flow_accumulation, sr, slope, TWI, cwd, vpd_aug, vpd_jan, aspect_transformed, ndvi, ndmi_2006) # the predictor$new code had issues with using a df with NA values even thought the variabels in the RF do not have NA values. 

train_x <- mcf_ale[-which(names(mcf_ale) == "two_class_mortality_mod")]

# predictors 
other_predictor <- Predictor$new(rfm, data = mcf_ale, y = mcf_ale$two_class_mortality_mod, type = "prob", class="Other")
high_predictor <- Predictor$new(rfm, data = mcf_ale, y = mcf_ale$two_class_mortality_mod, type = "prob", class="High")

#vpd_jan
other <- plot(FeatureEffect$new(other_predictor, feature = "ndvi")) + ggtitle("Other")
high <- plot(FeatureEffect$new(high_predictor, feature = "ndvi")) + ggtitle("High")
ggarrange(other, high)

#slope
other <- plot(FeatureEffect$new(other_predictor, feature = "slope")) + ggtitle("Other")
high <- plot(FeatureEffect$new(high_predictor, feature = "slope")) + ggtitle("High")
ggarrange(other, high)

#sr
other <- plot(FeatureEffect$new(other_predictor, feature = "sr")) + ggtitle("Other")
high <- plot(FeatureEffect$new(high_predictor, feature = "sr")) + ggtitle("High")
ggarrange(other, high)

```



# rdnbr: classification (2 classes)
```{r}
for (i in 1:10) {
  
rfm <- randomForest(rdnbr_qualitative2 ~ rain + tmax + TPI_five + dem_10m_zaca + flow_accumulation + sr + slope + TWI + cwd + vpd_aug + vpd_jan + aspect_transformed + ndvi + ndmi_2006, 
                    data = mcf, 
                    sampsize = c('High' = 150, 'Other' = 150), 
                    ntree=1500, 
                    importance=T,
                    replace = T)
# OOB and importance
rfm
importance(rfm)
#plot to visualize variable importance
varImpPlot(rfm)

}
```
get very similar oob and class accuracy as the manual mortality trial with 2 classes (none/low/moderate, high). So if we just look at high mortality v other mortality then we can use either rdnbr or manual mortality but if we chose to show the other classes it makes sense to use manual mortality since it is a lot more accurate, and thus more representative, than rdnbr. 

ROC and AUC
```{r}
# code: https://stats.stackexchange.com/questions/188616/how-can-we-calculate-roc-auc-for-classification-algorithm-such-as-random-forest
rfm_roc <-  roc(mcf$rdnbr_qualitative2, rfm$votes[,1])
plot(rfm_roc) # plot comes out a little weird
auc(rfm_roc)
```

PDP
```{r}
#PDP
c <- partialPlot(rfm, as.data.frame(mcf), vpd_jan, "Other") 
b <- partialPlot(rfm, as.data.frame(mcf), vpd_jan, "High", col="red", ylim=c(-0.8,0.8)) 
lines(c, col="green")

c <- partialPlot(rfm, as.data.frame(mcf), slope, "Other") 
b <- partialPlot(rfm, as.data.frame(mcf), slope, "High", col="red", ylim=c(-0.8,0.8)) 

c <- partialPlot(rfm, as.data.frame(mcf), sr, "Other") 
b <- partialPlot(rfm, as.data.frame(mcf), sr, "High", col="red") 

```

ALE
```{r}
#sub data
mcf_ale <- mcf %>% 
  select(rdnbr_qualitative2, rain, tmax, TPI_five, dem_10m_zaca, flow_accumulation, sr, slope, TWI, cwd, vpd_aug, vpd_jan, aspect_transformed, ndvi, ndmi_2006) # the predictor$new code had issues with using a df with NA values even thought the variabels in the RF do not have NA values. 

train_x <- mcf_ale[-which(names(mcf_ale) == "rdnbr_qualitative2")]

# predictors 
other_predictor <- Predictor$new(rfm, data = mcf_ale, y = mcf_ale$rdnbr_qualitative2, type = "prob", class="Other")
high_predictor <- Predictor$new(rfm, data = mcf_ale, y = mcf_ale$rdnbr_qualitative2, type = "prob", class="High")

#vpd_jan
other <- plot(FeatureEffect$new(other_predictor, feature = "ndvi")) + ggtitle("Other")
high <- plot(FeatureEffect$new(high_predictor, feature = "ndvi")) + ggtitle("High")
ggarrange(other, high)

#slope
other <- plot(FeatureEffect$new(other_predictor, feature = "slope")) + ggtitle("Other")
high <- plot(FeatureEffect$new(high_predictor, feature = "slope")) + ggtitle("High")
ggarrange(other, high)

#sr
other <- plot(FeatureEffect$new(other_predictor, feature = "sr")) + ggtitle("Not Type Converted")
high <- plot(FeatureEffect$new(high_predictor, feature = "sr")) + ggtitle("Type Converted")
ggarrange(other, high)

```
interesting. for rdnbr classification, the ale plots show different trends for ndvi. .However, this might change as the explanatory variables change

# rdnbr: regression
```{r}
for (i in 1:10) {
  
rfm <- randomForest(rdnbr_2007 ~ rain + tmax + TPI_five + dem_10m_zaca + flow_accumulation + sr + slope + TWI + cwd + vpd_aug + vpd_jan, 
                    data = mcf, 
                    #sampsize = c('1' = 99, '0' = 99), 
                    ntree=1500, 
                    importance=T,
                    replace = T)
# OOB and importance
rfm
importance(rfm)
#plot to visualize variable importance
varImpPlot(rfm)

}
```

PDP
```{r}
#PDP
partialPlot(rfm, as.data.frame(mcf), vpd_jan) 
partialPlot(rfm, as.data.frame(mcf), slope) 
partialPlot(rfm, as.data.frame(mcf), sr) 

```
not sure why the y axis doesnt go higher (the max rdnbr value is over 1400)

ALE
```{r}
#sub data
mcf_ale <- mcf %>% 
  select(rdnbr_qualitative2, rain, tmax, TPI_five, dem_10m_zaca, flow_accumulation, sr, slope, TWI, cwd, vpd_aug, vpd_jan, aspect_transformed) # the predictor$new code had issues with using a df with NA values even thought the variabels in the RF do not have NA values. 

train_x <- mcf_ale[-which(names(mcf_ale) == "rdnbr_qualitative")]

# predictors 
predictor <- Predictor$new(rfm, data = mcf_ale)

#explanatory variables
vpd_jan <- plot(FeatureEffect$new(predictor, feature = "vpd_jan")) + ggtitle("vpd_jan")
slope <- plot(FeatureEffect$new(predictor, feature = "slope")) + ggtitle("slope")
sr <- plot(FeatureEffect$new(predictor, feature = "sr")) + ggtitle("sr")
ggarrange(vpd_jan, slope, sr)

```
not sure why the y axis doesnt go higher (the max rdnbr value is over 1400)


# rdnbr: regression PICO
```{r}
for (i in 1:10) {
  
rfm <- randomForest(rdnbr_2007 ~ rain + tmax + TPI_five + dem  + insolation + slope + TWI + cwd + vpd_aug + vpd_jan + aspect_transformed, #missing fow accumulation
                    data = pico, 
                    #sampsize = c('1' = 99, '0' = 99), 
                    ntree=1500, 
                    importance=T,
                    replace = T)
# OOB and importance
rfm
importance(rfm)
#plot to visualize variable importance
varImpPlot(rfm)

}
```

PDP
```{r}
#PDP
partialPlot(rfm, as.data.frame(pico), rain) 
partialPlot(rfm, as.data.frame(pico), slope) 
partialPlot(rfm, as.data.frame(pico), insolation) 

```
not sure why the y axis doesnt go higher (the max rdnbr value is over 1400)

ALE
```{r}
#sub data
mcf_ale <- mcf %>% 
  select(rdnbr_qualitative2, rain, tmax, TPI_five, dem_10m_zaca, flow_accumulation, sr, slope, TWI, cwd, vpd_aug, vpd_jan, aspect_transformed) # the predictor$new code had issues with using a df with NA values even thought the variabels in the RF do not have NA values. 

train_x <- mcf_ale[-which(names(mcf_ale) == "rdnbr_qualitative")]

# predictors 
predictor <- Predictor$new(rfm, data = mcf_ale)

#explanatory variables
vpd_jan <- plot(FeatureEffect$new(predictor, feature = "vpd_jan")) + ggtitle("vpd_jan")
slope <- plot(FeatureEffect$new(predictor, feature = "slope")) + ggtitle("slope")
sr <- plot(FeatureEffect$new(predictor, feature = "sr")) + ggtitle("sr")
ggarrange(vpd_jan, slope, sr)

```
not sure why the y axis doesnt go higher (the max rdnbr value is over 1400)

# rdnbr: classification (2 classes) PICO
```{r}
for (i in 1:10) {
  
rfm <- randomForest(rdnbr_qualitative2 ~ rain + tmax + TPI_five + dem + insolation + slope + TWI + cwd + vpd_aug + vpd_jan + aspect_transformed + years_since_burn,
                    data = pico, 
                    sampsize = c('High' = 319, 'Other' = 319), 
                    ntree=1500, 
                    importance=T,
                    replace = T)
# OOB and importance
rfm
importance(rfm)
#plot to visualize variable importance
varImpPlot(rfm)

}
```
get very similar oob and class accuracy as the manual mortality trial with 2 classes (none/low/moderate, high). So if we just look at high mortality v other mortality then we can use either rdnbr or manual mortality but if we chose to show the other classes it makes sense to use manual mortality since it is a lot more accurate, and thus more representative, than rdnbr. 

ROC and AUC
```{r}
# code: https://stats.stackexchange.com/questions/188616/how-can-we-calculate-roc-auc-for-classification-algorithm-such-as-random-forest
rfm_roc <-  roc(pico$rdnbr_qualitative2, rfm$votes[,1])
plot(rfm_roc) # plot comes out a little weird
auc(rfm_roc)
```

PDP
```{r}
#PDP
c <- partialPlot(rfm, as.data.frame(pico), vpd_jan, "Other") 
b <- partialPlot(rfm, as.data.frame(pico), vpd_jan, "High", col="red", ylim=c(-0.8,0.8)) 
lines(c, col="green")

c <- partialPlot(rfm, as.data.frame(pico), insolation, "Other") 
b <- partialPlot(rfm, as.data.frame(pico), insolation, "High", col="red", ylim=c(-0.8,0.8)) 

c <- partialPlot(rfm, as.data.frame(pico), tmax, "Other") 
b <- partialPlot(rfm, as.data.frame(pico), tmax, "High", col="red") 

```

ALE
```{r}
#sub data
pico_ale <- pico %>% 
  select(rdnbr_qualitative2, rain, tmax, TPI_five, dem, insolation, slope, TWI, cwd, vpd_aug, vpd_jan, aspect_transformed) # the predictor$new code had issues with using a df with NA values even thought the variabels in the RF do not have NA values. 

train_x <- pico_ale[-which(names(pico_ale) == "rdnbr_qualitative2")]

# predictors 
other_predictor <- Predictor$new(rfm, data = pico_ale, y = pico_ale$rdnbr_qualitative2, type = "prob", class="Other")
high_predictor <- Predictor$new(rfm, data = pico_ale, y = pico_ale$rdnbr_qualitative2, type = "prob", class="High")

#rain
other <- plot(FeatureEffect$new(other_predictor, feature = "rain")) + ggtitle("Other")
high <- plot(FeatureEffect$new(high_predictor, feature = "rain")) + ggtitle("High")
ggarrange(other, high)

#temp
other <- plot(FeatureEffect$new(other_predictor, feature = "tmax")) + ggtitle("Other")
high <- plot(FeatureEffect$new(high_predictor, feature = "tmax")) + ggtitle("High")
ggarrange(other, high)

#sr
other <- plot(FeatureEffect$new(other_predictor, feature = "insolation")) + ggtitle("Not Type Converted")
high <- plot(FeatureEffect$new(high_predictor, feature = "insolation")) + ggtitle("Type Converted")
ggarrange(other, high)

```

# rdnbr: regression (mcf + pico)

sub data 
```{r}
# sub data: combine mcf and pico dfs ----------------------

# pico sub
pico_sub <- pico %>% 
  rename(sr = insolation,
         dem_10m_zaca = dem) %>% 
  select(-ORIG_FID, -aspect,-years_since_burn, -tmin, -tmean, -TPI_queen, -dnbr_2007, -ndmi_2007, -dnbr_qualitative) #missing flow_accumulation

# mcf sub
mcf_sub <- mcf %>% 
  select(rain, tmax,  TPI_five , dem_10m_zaca , sr , slope , TWI , cwd , vpd_aug , vpd_jan , aspect_transformed, rdnbr_2007, rdnbr_qualitative2, rdnbr_qualitative)
  
# combine
all <- rbind(pico_sub, mcf_sub)

# sample size
rdnbr_all_4 <- all %>% 
  group_by(rdnbr_qualitative) %>% 
  count()
```
high: 863, moderate: 459, low: 437, none: 8

regression
```{r}
for (i in 1:10) {
  
rfm <- randomForest(rdnbr_2007 ~ rain + tmax + TPI_five + dem_10m_zaca + sr + slope + TWI + cwd + vpd_aug + vpd_jan + aspect_transformed, 
                    data = all, 
                    #sampsize = c('1' = 99, '0' = 99), 
                    ntree=1500, 
                    importance=T,
                    replace = T)
# OOB and importance
rfm
importance(rfm)
#plot to visualize variable importance
varImpPlot(rfm)

}
```

PDP
```{r}
#PDP
partialPlot(rfm, as.data.frame(all), rain) 
partialPlot(rfm, as.data.frame(all), tmax) 
partialPlot(rfm, as.data.frame(all), sr) 

```
not sure why the y axis doesnt go higher (the max rdnbr value is over 1400)

ALE
```{r}
#sub data
all_ale <- all %>% 
  select(rdnbr_qualitative2, rain, tmax, TPI_five, dem_10m_zaca, sr, slope, TWI, cwd, vpd_aug, vpd_jan, aspect_transformed) # the predictor$new code had issues with using a df with NA values even thought the variabels in the RF do not have NA values. 

train_x <- all_ale[-which(names(all_ale) == "rdnbr_qualitative")]

# predictors 
predictor <- Predictor$new(rfm, data = all_ale)

#explanatory variables
vpd_jan <- plot(FeatureEffect$new(predictor, feature = "vpd_jan")) + ggtitle("vpd_jan")
slope <- plot(FeatureEffect$new(predictor, feature = "slope")) + ggtitle("slope")
sr <- plot(FeatureEffect$new(predictor, feature = "sr")) + ggtitle("sr")
ggarrange(vpd_jan, slope, sr)

```

# rdnbr: classification (mcf + pico)
```{r}
for (i in 1:10) {
  
rfm <- randomForest(rdnbr_qualitative2 ~ rain + tmax + TPI_five + dem_10m_zaca + sr + slope + TWI + cwd + vpd_aug + vpd_jan + aspect_transformed,
                    data = all, 
                    sampsize = c('High' = 863, 'Other' = 863), 
                    ntree=1500, 
                    importance=T,
                    replace = T)
# OOB and importance
rfm
importance(rfm)
#plot to visualize variable importance
varImpPlot(rfm)

}
```

ROC and AUC
```{r}
# code: https://stats.stackexchange.com/questions/188616/how-can-we-calculate-roc-auc-for-classification-algorithm-such-as-random-forest
rfm_roc <-  roc(all$rdnbr_qualitative2, rfm$votes[,1])
plot(rfm_roc) # plot comes out a little weird
auc(rfm_roc)
```

PDP
```{r}
#PDP
c <- partialPlot(rfm, as.data.frame(all), tmax, "Other") 
b <- partialPlot(rfm, as.data.frame(all), tmax, "High", col="red", ylim=c(-0.8,0.8)) 
lines(c, col="green")

c <- partialPlot(rfm, as.data.frame(all), rain, "Other") 
b <- partialPlot(rfm, as.data.frame(all), rain, "High", col="red", ylim=c(-0.8,0.8)) 
lines(c, col="green")

c <- partialPlot(rfm, as.data.frame(all), sr, "Other") 
b <- partialPlot(rfm, as.data.frame(all), sr, "High", col="red") 
lines(c, col="green")

```

ALE
```{r}
#sub data
all_ale <- all %>% 
  select(rdnbr_qualitative2, rain, tmax, TPI_five, dem_10m_zaca, sr, slope, TWI, cwd, vpd_aug, vpd_jan, aspect_transformed) # the predictor$new code had issues with using a df with NA values even thought the variabels in the RF do not have NA values. 

train_x <- all_ale[-which(names(all_ale) == "rdnbr_qualitative2")]

# predictors 
other_predictor <- Predictor$new(rfm, data = all_ale, y = all_ale$rdnbr_qualitative2, type = "prob", class="Other")
high_predictor <- Predictor$new(rfm, data = all_ale, y = all_ale$rdnbr_qualitative2, type = "prob", class="High")

#vpd_jan
other <- plot(FeatureEffect$new(other_predictor, feature = "tmax")) + ggtitle("Other")
high <- plot(FeatureEffect$new(high_predictor, feature = "tmax")) + ggtitle("High")
ggarrange(other, high)

#slope
other <- plot(FeatureEffect$new(other_predictor, feature = "rain")) + ggtitle("Other")
high <- plot(FeatureEffect$new(high_predictor, feature = "rain")) + ggtitle("High")
ggarrange(other, high)

#sr
other <- plot(FeatureEffect$new(other_predictor, feature = "sr")) + ggtitle("Other")
high <- plot(FeatureEffect$new(high_predictor, feature = "sr")) + ggtitle("High")
ggarrange(other, high)

```

# Type conversion: manual mortality (classification)

0 = not type converted
1 = type converted
```{r}

for (i in 1:10) {
  
rfm <- randomForest(tc_mortality ~ rain + tmax + TPI_five + dem_10m_zaca + flow_accumulation + sr + slope + TWI + cwd + vpd_aug + vpd_jan + aspect_transformed + ndvi, 
                    data = mcf, 
                    #sampsize = c('1' = 99, '0' = 99), 
                    ntree=1500, 
                    importance=T,
                    replace = T)
# OOB and importance
rfm
importance(rfm)
#plot to visualize variable importance
varImpPlot(rfm)

}

```


ROC and AUC
```{r}
# code: https://stats.stackexchange.com/questions/188616/how-can-we-calculate-roc-auc-for-classification-algorithm-such-as-random-forest
rfm_roc <-  roc(mcf$tc_mortality, rfm$votes[,1])
plot(rfm_roc) # plot comes out a little weird
auc(rfm_roc)
```

PDP
```{r}
#PDP
a <- partialPlot(rfm, as.data.frame(mcf), vpd_jan, "0") #0=not type converted
b <- partialPlot(rfm, as.data.frame(mcf), vpd_jan, "1", col="red") #1=type converted
lines(a, col="green")

a <- partialPlot(rfm, as.data.frame(mcf), slope, "0") #0=not type converted
b <- partialPlot(rfm, as.data.frame(mcf), slope, "1", col="red") #1=type converted
lines(a, col="green")

a <- partialPlot(rfm, as.data.frame(mcf), sr, "0") #0=not type converted
b <- partialPlot(rfm, as.data.frame(mcf), sr, "1", col="red") #1=type converted
lines(a, col="green")

```
so looking at the 3 most important variables for type conversino plots, all of the probabilities are below zero. only the not type converted stands have probabilities above 0, with some values of the explanatory variable having very high probabilities. So what this tells me is the model CANNOT distinguish type converted stands from non type converted stands. 

ALE
```{r}
#sub data
mcf_ale <- mcf %>% 
  select(tc_mortality, rain, tmax, TPI_five, dem_10m_zaca, flow_accumulation, sr, slope, TWI, cwd, vpd_aug, vpd_jan) # the predictor$new code had issues with using a df with NA values even thought the variabels in the RF do not have NA values. 

train_x <- mcf_ale[-which(names(mcf_ale) == "tc_mortality")]

# predictors 
nottc_predictor <- Predictor$new(rfm, data = mcf_ale, y = mcf_ale$tc_mortality, type = "prob", class="0")
tc_predictor <- Predictor$new(rfm, data = mcf_ale, y = mcf_ale$tc_mortality, type = "prob", class="1")

#vpd_jan
other <- plot(FeatureEffect$new(nottc_predictor, feature = "vpd_jan")) + ggtitle("Not Type Converted")
high <- plot(FeatureEffect$new(tc_predictor, feature = "vpd_jan")) + ggtitle("Type Converted")
ggarrange(other, high)

#slope
other <- plot(FeatureEffect$new(nottc_predictor, feature = "slope")) + ggtitle("Not Type Converted")
high <- plot(FeatureEffect$new(tc_predictor, feature = "slope")) + ggtitle("Type Converted")
ggarrange(other, high)

#sr
other <- plot(FeatureEffect$new(nottc_predictor, feature = "sr")) + ggtitle("Not Type Converted")
high <- plot(FeatureEffect$new(tc_predictor, feature = "sr")) + ggtitle("Type Converted")
ggarrange(other, high)

```


# Type conversion: rdnbr threshold (classification)

preliminary threshold: 1160
```{r}

for (i in 1:10) {
  
rfm <- randomForest(rdnbr_tc_threshold ~ rain + tmax + TPI_five + dem_10m_zaca + flow_accumulation + sr + slope + TWI + cwd + vpd_aug + vpd_jan + aspect_transformed + ndvi, 
                    data = tc_threshold, 
                    #sampsize = c('1' = 99, '0' = 99), 
                    ntree=1500, 
                    importance=T,
                    replace = T)
# OOB and importance
rfm
importance(rfm)
#plot to visualize variable importance
varImpPlot(rfm)

}
```
ROC and AUC
```{r}
# code: https://stats.stackexchange.com/questions/188616/how-can-we-calculate-roc-auc-for-classification-algorithm-such-as-random-forest
rfm_roc <-  roc(tc_threshold$rdnbr_tc_threshold, rfm$votes[,1])
plot(rfm_roc) # plot comes out a little weird
auc(rfm_roc)
```

PDP
```{r}
#PDP
a <- partialPlot(rfm, as.data.frame(tc_threshold), rain, "not_tc") 
b <- partialPlot(rfm, as.data.frame(tc_threshold), rain, "tc", col="red") 
lines(a, col="green")

a <- partialPlot(rfm, as.data.frame(tc_threshold), slope, "not_tc") 
b <- partialPlot(rfm, as.data.frame(tc_threshold), slope, "tc", col="red") 
lines(a, col="green")

a <- partialPlot(rfm, as.data.frame(tc_threshold), vpd_aug, "not_tc") 
b <- partialPlot(rfm, as.data.frame(tc_threshold), vpd_aug, "tc", col="red") 
lines(a, col="green")

```
ALE
```{r}
#sub data
tc_threshold_ale <- tc_threshold %>% 
  select(rdnbr_tc_threshold, rain, tmax, TPI_five, dem_10m_zaca, flow_accumulation, sr, slope, TWI, cwd, vpd_aug, vpd_jan, aspect_transformed, ndvi) # the predictor$new code had issues with using a df with NA values even thought the variabels in the RF do not have NA values. 

train_x <- tc_threshold_ale[-which(names(tc_threshold_ale) == "rdnbr_tc_threshold")]

# predictors 
nottc_predictor <- Predictor$new(rfm, data = tc_threshold_ale, y = tc_threshold_ale$rdnbr_tc_threshold, type = "prob", class="not_tc")
tc_predictor <- Predictor$new(rfm, data = tc_threshold_ale, y = tc_threshold_ale$rdnbr_tc_threshold, type = "prob", class="tc")

#vpd_jan
other <- plot(FeatureEffect$new(nottc_predictor, feature = "vpd_jan")) + ggtitle("Not Type Converted")
high <- plot(FeatureEffect$new(tc_predictor, feature = "vpd_jan")) + ggtitle("Type Converted")
ggarrange(other, high)

#slope
other <- plot(FeatureEffect$new(nottc_predictor, feature = "slope")) + ggtitle("Not Type Converted")
high <- plot(FeatureEffect$new(tc_predictor, feature = "slope")) + ggtitle("Type Converted")
ggarrange(other, high)

#sr
other <- plot(FeatureEffect$new(nottc_predictor, feature = "rain")) + ggtitle("Not Type Converted")
high <- plot(FeatureEffect$new(tc_predictor, feature = "rain")) + ggtitle("Type Converted")
ggarrange(other, high)

```

### ALE Plots

So I was using partial dependence plots to visualize the variables in RF, but then found out you shouldn't use PDP when the variables are correlated with other variables it's one of the assumptions of PDP https://christophm.github.io/interpretable-ml-book/pdp.html ; https://arxiv.org/pdf/1612.08468.pdf). Instead, I should be using Accumulated Local Effects (ALE) plots (https://christophm.github.io/interpretable-ml-book/ale.html). May also want to look at ICE plots (https://towardsdatascience.com/an-overview-of-model-explainability-in-modern-machine-learning-fc0f22c8c29a)

- code example of how to make ALE using iml package:
  - https://medium.com/the-die-is-forecast/shining-a-light-on-the-black-box-of-machine-learning-2b49fe471cee
  - https://cran.r-project.org/web/packages/iml/vignettes/intro.html

- webpage explaining ALE: https://christophm.github.io/interpretable-ml-book/ale.html
- github code for webpage above: https://github.com/christophM/interpretable-ml-book/blob/6153b2ac0143f879bd4a856a1048545a40cd1a74/manuscript/05.4-agnostic-ale.Rmd

other sources (1-2 journal article)
- https://towardsdatascience.com/an-overview-of-model-explainability-in-modern-machine-learning-fc0f22c8c29a
- https://www.moodysanalytics.com/-/media/presentation/2019/mas-t1d1-transparent-and-interpretable-ml-in-risk-modeling.pdf
- https://www.enjine.com/blog/interpreting-machine-learning-models-accumulated-local-effects/ --> it would be cool to be able to make the plot in this website
- https://arxiv.org/pdf/1612.08468.pdf
- https://cran.r-project.org/web/packages/iml/iml.pdf
- https://cran.r-project.org/web/packages/iml/vignettes/intro.html
- **https://rdrr.io/cran/iml/src/R/FeatureEffect.R --> has citation
- https://cran.r-project.org/web/packages/ALEPlot/ALEPlot.pdf (iml package and documentation a lot more useful)

So, I just wanted to get the code started. I think you will be better at streamlining makingthe rest of the graphs, especially with the same axis scales. One thing I did notice is that ALE plots for the same mortality class resulted in 

```{r}

## sample code to make ALE plos (using solar radiation as example) used combo of: https://cran.r-project.org/web/packages/iml/vignettes/intro.html and https://cran.r-project.org/web/packages/iml/iml.pdf

#do something with the response variable column (removes it but R can still recognize it)
train_x <- train[-which(names(train) == "mortality_qual")]

# this does something to create values that will be plotted in the ALE plot: inputs include the random forest model (rfm) run above, the trainig data with the modified response column (created above), specifying we want output to be a probability, looking at a specific mortality class 
sr_predictor_high <- Predictor$new(rfm, data = train_x, type = "prob", class = "High")

# plot the outputs in an ALE plot using the predictor info created above just for solar radiation variable(feature = "explanatory variable"). Doesnt work if do featureeffects$new() and then plot() on separate lines even though thats how one of the smaple codes showed it. 
sr_high <- plot(FeatureEffect$new(sr_predictor_high, feature = "sr")) + ggtitle("High")

#predict values for the other mortality classes for the same variable
sr_predictor_moderate <- Predictor$new(rfm, data = train_x, type = "prob", class = "Moderate")
sr_moderate <- plot(FeatureEffect$new(sr_predictor_moderate, feature = "sr")) + ggtitle("Moderate")

sr_predictor_low <- Predictor$new(rfm, data = train_x, type = "prob", class = "Low")
sr_low <- plot(FeatureEffect$new(sr_predictor_low, feature = "sr")) + ggtitle("Low")

sr_predictor_none <- Predictor$new(rfm, data = train_x, type = "prob", class = "None")
sr_none <- plot(FeatureEffect$new(sr_predictor_none, feature = "sr")) + ggtitle("None")

ggarrange(sr_high, sr_moderate, sr_low, sr_none, ncol = 2, nrow = 2)

#plot all variables(for just one mortality class) : for some reason the function makes graphs for all the variables in the data frame, not just the ones in the random forest model. 
tt <- FeatureEffects$new(sr_predictor_high, method = "ale")
plot(tt)

# plot all variables in the model (for just one mortality class): had to specify all the individual variables in the model to plot from the train data frame
all_rfm_variables <- FeatureEffects$new(sr_predictor_low, method = "ale", features = c("cwd", "rain", 'tmax' , 'TPI_five' , 'dem_10m_zaca' , 'flow_accumulation' , 'sr' ,'slope', 'TWI', 'vpd_aug'))
all_rfm_variables$plot()
ggsave("ale_low.jpg", height = 9, width = 9)

# plot a variables in ALE plot: https://medium.com/the-die-is-forecast/shining-a-light-on-the-black-box-of-machine-learning-2b49fe471cee
plot(FeatureEffect$new(sr_predictor_low, feature = c("cwd", "dem_10m_zaca")))

# another way to make ale plts with iml package (doesnt work): https://medium.com/the-die-is-forecast/shining-a-light-on-the-black-box-of-machine-learning-2b49fe471cee
sr_predictor_low <- Predictor$new(rfm, data = train_x, type = "prob", class = "Low")
ale_iml <- FeatureEffect$new(sr_predictor_low, feature = "sr")
sr_df <- ale_iml$results
ggplot(sr_df, aes(x = sr, y = .ale)) + geom_line() # not sure what .ale is. the samle code doesn't say, but the above codes work so whatever

# another way to write the code: https://github.com/christophM/interpretable-ml-book/blob/6153b2ac0143f879bd4a856a1048545a40cd1a74/manuscript/05.4-agnostic-ale.Rmd
FeatureEffects$new(sr_predictor_low, feature = "sr", method = "ale")$plot() +ggtitle("Low") #produces the same graph as above


# categorical variable 
none <-  plot(FeatureEffect$new(none_predictor, feature = "rdnbr_qualitative")) + ggtitle("None")
low <- plot(FeatureEffect$new(low_predictor, feature = "rdnbr_qualitative")) + ggtitle("Low")
moderate <- plot(FeatureEffect$new(moderate_predictor, feature = "rdnbr_qualitative")) + ggtitle("Moderate")
high <- plot(FeatureEffect$new(high_predictor, feature = "rdnbr_qualitative")) + ggtitle("High")
ggarrange(none, low, moderate, high)

# plot 2 variables
none <-  plot(FeatureEffect$new(none_predictor, feature = c("tmax", "rain"))) + ggtitle("None")
low <- plot(FeatureEffect$new(low_predictor, feature = c("tmax", "rain"))) + ggtitle("Low")
moderate <- plot(FeatureEffect$new(moderate_predictor, feature = c("tmax", "rain"))) + ggtitle("Moderate")
high <- plot(FeatureEffect$new(high_predictor, feature = c("tmax", "rain"))) + ggtitle("High")
ggarrange(none, low, moderate, high)




```
- get error when try to run the model on the test data instead of the train data. Think the data used in the model, needs to be the same data to do ALE plots.

- ale plots let us determine the effect that each individual input, isolated/regardless of all others, has on our output.

- How to interpret ALE plots: not really sure since the papers/tutorials I've seen show how to interpret grpahs for quantitative response variables, but I assume since we indicate "type = prob" in the code that the y axis is the probability of a stand having that mortality class at specific value of the x variable

- By localizing the measurements via the use of windows, we are able to avoid including practically unlikely or impossible situations. For instance, in our model to predict the number of joggers in a day, temperature and time of year are likely highly correlated. By limiting our window in our ALE plot, we can ensure that we're not factoring in a scenario where the time of year is winter and the temperature is 35 degrees Celsius. We shouldn't be factoring in the model's prediction for this situation because it is certainly outside of any data set we will be using. ALE plots are able to avoid such situations and give us much more accurate results.



####  (ignore below)



### Random Forest and forced sample sizes with training and test data

hmmmm. not sure how to do this with training and test data since the train/test data frames will have different quantities of the type conversion classes each time. if i do it the way I have been doing it below, then variable importance is too unstable. even using my equal distribution sample size code and adding in the sampsize function results in unstable variable importance. 
```{r}
#train and test data 
sample <- sample(1:nrow(mcf), size = 0.75*nrow(mcf))
train <- mcf[sample, ]
test  <- mcf[-sample, ]
 
 
 
for (i in 1:10) {
rfm <- randomForest(tc_mortality ~ rain + tmax + TPI_five + dem_10m_zaca + flow_accumulation + sr + slope + TWI + cwd + vpd_aug + vpd_jan + drainage_association + ndvi, 
                    data = mcf, 
                    sampsize = c('1' = 99, '0' = 99), 
                    ntree=1500, 
                    importance=T,
                    replace = T)
rfm

#plot to visualize variable importance
varImpPlot(rfm)
#model_importance <- importance(rfm)
importance(rfm)
}
#predcit test data
pp<-predict(rfm, test)
  
#test data accuracy
tabb<- table(pp, test$tc_mortality)
acc<-1-sum(diag(tabb))/sum(tabb) # % missclassified 
test_accuracy <-(1-acc)*100 #model accuracy
```


### Random Forest: ALL observations

to get a stable variable importance. The other models result in very unstable variable importance cmeasures which shuldnt happen with RF but is occuring bc the training data used for each run changes. 

```{r}
#four class mortality
for (i in 1:10) {
  
rfm <- randomForest(mortality_qual ~ rain + tmax + TPI_five + dem_10m_zaca + flow_accumulation + sr + slope + TWI + cwd + vpd_aug + ndvi, data = mcf, ntree=1500, importance=T, nscale=0)

rfm  

#plot to visualize variable importance
varImpPlot(rfm)
model_importance <- importance(rfm)

}

#3 class mortality
for (i in 1:10) {
  
rfm <- randomForest(three_class_mortality ~ rain + tmax + TPI_five + dem_10m_zaca + flow_accumulation + sr + slope + TWI + cwd + vpd_aug + ndvi, data = mcf, ntree=1500, importance=T, nscale=0)

rfm  

#plot to visualize variable importance
varImpPlot(rfm)
model_importance <- importance(rfm)

}

```
So 1500 trees reuslts in more stable importance values for each model. The model with 3 mortality classes has a better OOB.


# Random Forest: 4 mortality classes
```{r}
# model parameters---------------------------------------------

# set sample size 
nsample = 125

# lists to save outputs from for loop
test_accuracy_list <- list()

```


```{r}

#random forest code---------------------------------------------------
for (i in 1:500)
{
  
  # divide data into equal # of observations --------------------
  
  mcf_high <- mcf[which(mcf$mortality_qual == "High"),]
  mcf_low <- mcf[which(mcf$mortality_qual == "Low"),]
  mcf_moderate <- mcf[which(mcf$mortality_qual == "Moderate"),]
  mcf_none <- mcf[which(mcf$mortality_qual == "None"),]
  
  # pull random observations from the high class and separate
  x_high <- sample(1:nrow(mcf_high), nsample , replace = FALSE)
  polygon_data_high_train <- mcf_high[x_high,]
  polygon_data_high_test <- mcf_high[-x_high,]
  # pull random observations from the low class and separate
  x_low <- sample(1:nrow(mcf_low), nsample , replace = FALSE)
  polygon_data_low_train <- mcf_low[x_low,]
  polygon_data_low_test <- mcf_low[-x_low,]
  # pull random observations from the moderate class and separate
  x_moderate <- sample(1:nrow(mcf_moderate), nsample , replace = FALSE)
  polygon_data_moderate_train <- mcf_moderate[x_moderate,]
  polygon_data_moderate_test <- mcf_moderate[-x_moderate,]
  # pull random observations from the none class and separate
  x_none <- sample(1:nrow(mcf_none), nsample , replace = FALSE)
  polygon_data_none_train <- mcf_none[x_none,]
  polygon_data_none_test <- mcf_none[-x_none,]
  
  # combine all random observations
  train <- do.call("rbind", list(polygon_data_none_train, polygon_data_low_train, polygon_data_moderate_train, polygon_data_high_train))
  # recombine classes that were not pulled
  test <- do.call("rbind", list(polygon_data_none_test, polygon_data_low_test, polygon_data_moderate_test, polygon_data_high_test))
  
  #random forest model
  rfm<-randomForest(mortality_qual~ rain + tmax + TPI_five + dem_10m_zaca + flow_accumulation + sr + slope + TWI + cwd + vpd_aug + ndvi, data = train, ntree=500, importance=T, nscale=0)
  
  #plot to visualize variable importance
  #varImpPlot(rfm)
  model_importance <- importance(rfm)

  #predcit test data
  pp<-predict(rfm, test)
  
  #test data accuracy
  tabb<- table(pp, test$mortality_qual)
  acc<-1-sum(diag(tabb))/sum(tabb) #% missclassified 
  test_accuracy <-(1-acc)*100 #model accuracy
  
  # lists to save model outputs
  test_accuracy_list <- append(test_accuracy_list, test_accuracy)
  write.table(model_importance, file="rfm model importance.csv", sep=",",col.names=NA, append=T) 
  write.table(tabb, file = "mcf test accuracy table.csv", sep=",",col.names=NA, append=T)
}


```

- Note: when running the model, have to remember to delete the rfm model importance.csv and mcf test accuracy table.csv files otherwise R will keep adding the values onto the orignial file and not make a new one for each model succession. I've been doing that or renaming the file, but maybe theres an easier method
- QUESTION: Should I be using set seed? 

Model Results
```{r}

# overall average model accuracy -----------------------------------------------------

accuracy_df <- as.data.frame(unlist(test_accuracy_list))
mean_test_accuracy_1 <- accuracy_df %>% 
  mutate(test_accuracy_list = as.numeric(test_accuracy_list)) %>% 
  summarize(mean_accuracy = mean(unlist(test_accuracy_list)))


#average model importance ---------------------------------------------------------

model_importance_tidy <- read_csv(here("Model Data Sheets and Code", "rfm model importance.csv")) %>% 
  clean_names() %>% 
  filter(none != "None") %>% 
  rename (variables = x1) %>% 
  mutate (variables = str_to_lower(variables)) %>% 
  mutate (none = as.numeric(none),
          low = as.numeric(low),
          moderate = as.numeric(moderate),
          high = as.numeric(high),
          mean_decrease_accuracy = as.numeric(mean_decrease_accuracy),
          mean_decrease_gini = as.numeric(mean_decrease_gini)
          )

# model importance summary statistics
model_importance_summary <- model_importance_tidy %>% 
  group_by(variables) %>% 
  summarize (none_mean = mean(none),
             low_mean = mean(low),
             moderate_mean = mean(moderate),
             high_mean = mean(high),
             mean_dec_accuracy_mean = mean(mean_decrease_accuracy),
             mean_decrease_gini_mean = mean(mean_decrease_gini)
  )

file.remove(here::here("Model Data Sheets and Code", "rfm model importance.csv"))
# average class accuracy ---------------------------------------------------------

high <- read_csv("mcf test accuracy table.csv")
moderate <- read_csv("mcf test accuracy table.csv") 
low <- read_csv("mcf test accuracy table.csv")
none <- read_csv("mcf test accuracy table.csv")

high_mean_acc <- high %>% 
  select (X1, High) %>% 
  drop_na() %>% 
  filter (X1 == "High") %>% 
  mutate (High = as.numeric(High),
          class_acc = High / 33) %>% 
  summarise(mean = mean(class_acc))

moderate_mean_acc <- moderate %>% 
  select (X1, Moderate) %>% 
  drop_na() %>% 
  filter (X1 == "Moderate") %>%
  mutate (Moderate = as.numeric(Moderate),
          class_acc = Moderate / 35)%>% 
  summarise(mean = mean(class_acc))

low_mean_acc <- low %>% 
  select (X1, Low) %>% 
  drop_na() %>% 
  filter (X1 == "Low") %>%
  mutate (Low = as.numeric(Low),
          class_acc = Low / 41) %>% 
  summarise(mean = mean(class_acc))

none_mean_acc <- none %>% 
  select (X1, None) %>% 
  drop_na() %>% 
  filter (X1 == "None") %>%
  mutate (Low = as.numeric(None),
          class_acc = Low / 126) %>% 
  summarise(mean = mean(class_acc))

file.remove(here::here("Model Data Sheets and Code", "mcf test accuracy table.csv"))

```


### Random Forest Model: 3 mortality classes

No and low mortality are pretty similar and both would be ideal for land managers. 

```{r}
# model parameters ------------------------------

# set sample size 
nsample1 = 125

# lists to save outputs from for loop
test_accuracy_list <- list()
```


```{r}

#random forest code---------------------------------------------------
for (i in 1:5)
{
  
  # divide data into equal # of observations --------------------
  
  mcf_high <- mcf[which(mcf$three_class_mortality == "High"),]
  mcf_low <- mcf[which(mcf$three_class_mortality == "Low"),]
  mcf_moderate <- mcf[which(mcf$three_class_mortality == "Moderate"),]
  
  # pull random observations from the high class and separate
  x_high <- sample(1:nrow(mcf_high), nsample1 , replace = FALSE)
  polygon_data_high_train <- mcf_high[x_high,]
  polygon_data_high_test <- mcf_high[-x_high,]
  # pull random observations from the low class and separate
  x_low <- sample(1:nrow(mcf_low), nsample1 , replace = FALSE)
  polygon_data_low_train <- mcf_low[x_low,]
  polygon_data_low_test <- mcf_low[-x_low,]
  # pull random observations from the moderate class and separate
  x_moderate <- sample(1:nrow(mcf_moderate), nsample1 , replace = FALSE)
  polygon_data_moderate_train <- mcf_moderate[x_moderate,]
  polygon_data_moderate_test <- mcf_moderate[-x_moderate,]

  # combine all random observations
  train <- do.call("rbind", list(polygon_data_low_train, polygon_data_moderate_train, polygon_data_high_train))
  # recombine classes that were not pulled
  test <- do.call("rbind", list(polygon_data_low_test, polygon_data_moderate_test, polygon_data_high_test))
  
  #random forest model
  rfm<-randomForest(three_class_mortality~ rain + tmax + TPI_five + dem_10m_zaca + ndvi + sr + slope + TWI + cwd + vpd_aug, data = train, ntree=500, importance=T, nscale=0)

  #plot to visualize variable importance
  model_importance <- importance(rfm)
  varImpPlot(rfm)
  
  #predcit test data
  pp<-predict(rfm, test)
  
  #test data accuracy
  tabb<- table(pp, test$three_class_mortality)
  acc<-1-sum(diag(tabb))/sum(tabb) #% missclassified
  test_accuracy <-(1-acc)*100 #model accuracy
  
  # lists to save model accuracys
  test_accuracy_list <- append(test_accuracy_list, test_accuracy)
  write.table(model_importance, file="rfm model importance_3 class mortality.csv", sep=",",col.names=NA, append=T) 
  write.table(tabb, file = "mcf test accuracy table_3 class mortality.csv", sep=",",col.names=NA, append=T)
  
}

```

Model Results
```{r}

# overall average model accuracy -----------------------------------------------------

accuracy_df <- as.data.frame(unlist(test_accuracy_list))
mean_test_accuracy_1 <- accuracy_df %>% 
  summarize(mean_accuracy = mean(unlist(test_accuracy_list)))

#average model importance ---------------------------------------------------------

model_importance_tidy <- read_csv(here("Model Data Sheets and Code", "rfm model importance_3 class mortality.csv")) %>% 
  clean_names() %>% 
  filter(low != "Low") %>% 
  rename (variables = x1) %>% 
  mutate (variables = str_to_lower(variables)) %>% 
  mutate (low = as.numeric(low),
          moderate = as.numeric(moderate),
          high = as.numeric(high),
          mean_decrease_accuracy = as.numeric(mean_decrease_accuracy),
          mean_decrease_gini = as.numeric(mean_decrease_gini)
          )

# model importance summary statistics
model_importance_summary <- model_importance_tidy %>% 
  group_by(variables) %>% 
  summarize (low_mean = mean(low),
             moderate_mean = mean(moderate),
             high_mean = mean(high),
             mean_dec_accuracy_mean = mean(mean_decrease_accuracy),
             mean_decrease_gini_mean = mean(mean_decrease_gini)
  )
file.remove(here::here("Model Data Sheets and Code", "rfm model importance_3 class mortality.csv"))
# average class accuracy ---------------------------------------------------------

high <- read_csv("mcf test accuracy table_3 class mortality.csv") 
moderate <- read_csv("mcf test accuracy table_3 class mortality.csv")
low <- read_csv("mcf test accuracy table_3 class mortality.csv")  

high_mean_acc <- high %>% 
  select (X1, High) %>% 
  drop_na() %>% 
  filter (X1 == "High") %>% 
  mutate (High = as.numeric(High),
          class_acc = High / 33) %>% 
  summarise(mean = mean(class_acc))

moderate_mean_acc <- moderate %>% 
  select (X1, Moderate) %>% 
  drop_na() %>% 
  filter (X1 == "Moderate") %>%
  mutate (Moderate = as.numeric(Moderate),
          class_acc = Moderate / 35)%>% 
  summarise(mean = mean(class_acc))

low_mean_acc <- low %>% 
  select (X1, Low) %>% 
  drop_na() %>% 
  filter (X1 == "Low") %>%
  mutate (Low = as.numeric(Low),
          class_acc = Low / 292) %>% 
  summarise(mean = mean(class_acc))

file.remove(here::here("Model Data Sheets and Code", "mcf test accuracy table_3 class mortality.csv"))

```


ALE Plots
```{r}
train_x <- train[-which(names(train) == "three_class_mortality")]

sr_predictor_high <- Predictor$new(rfm, data = train_x, type = "prob", class = "High")
sr_high <- plot(FeatureEffect$new(sr_predictor_high, feature = "sr")) + ggtitle("High")

sr_predictor_moderate <- Predictor$new(rfm, data = train_x, type = "prob", class = "Moderate")
sr_moderate <- plot(FeatureEffect$new(sr_predictor_moderate, feature = "sr")) + ggtitle("Moderate")

sr_predictor_low <- Predictor$new(rfm, data = train_x, type = "prob", class = "Low")
sr_low <- plot(FeatureEffect$new(sr_predictor_low, feature = "sr")) + ggtitle("Low")

ggarrange(sr_high, sr_moderate, sr_low, ncol = 2, nrow = 2)

# all rfm variables
all_rfm_variables <- FeatureEffects$new(sr_predictor_low, method = "ale", features = c("cwd", "rain", 'tmax' , 'TPI_five' , 'dem_10m_zaca' , 'flow_accumulation' , 'sr' ,'slope', 'TWI', 'vpd_aug'))
all_rfm_variables$plot()
ggsave("ale_low_3.jpg", height = 9, width = 9)


```

### Random Forest 


```{r}
# model parameters---------------------------------------------

mcf_mod <- mcf %>% 
  mutate(tc_mortality = case_when(
    tc_mortality == 1 ~ "tc", #tc = type converted
    tc_mortality == 0 ~ "not_tc"
  )) %>% 
  mutate(tc_mortality = as.factor(tc_mortality))

# set sample size 
nsample = 75

# lists to save outputs from for loop
test_accuracy_list <- list()

```


```{r}

#random forest code---------------------------------------------------
for (i in 1:15)
{
  
  # divide data into equal # of observations --------------------
  mcf_high <- mcf_mod[which(mcf_mod$tc_mortality == "tc"),]
  mcf_low <- mcf_mod[which(mcf_mod$tc_mortality == "not_tc"),]

  # pull random observations from the high class and separate
  x_high <- sample(1:nrow(mcf_high), nsample , replace = FALSE)
  polygon_data_high_train <- mcf_high[x_high,]
  polygon_data_high_test <- mcf_high[-x_high,]
  # pull random observations from the low class and separate
  x_low <- sample(1:nrow(mcf_low), nsample , replace = FALSE)
  polygon_data_low_train <- mcf_low[x_low,]
  polygon_data_low_test <- mcf_low[-x_low,]

  
  # combine all random observations
  train <- do.call("rbind", list(polygon_data_low_train, polygon_data_high_train))
  # recombine classes that were not pulled
  test <- do.call("rbind", list(polygon_data_low_test, polygon_data_high_test))
  
  #random forest model
  rfm <- randomForest(tc_mortality ~ rain + tmax + TPI_five + dem_10m_zaca + flow_accumulation + sr + slope + TWI + cwd + vpd_aug + ridge + drainage_association + ndvi, data = train, ntree=1500, importance=T, nscale=0, sampsize = c('tc' = 75, 'not_tc' = 75), replace = T)
  
  #plot to visualize variable importance
  varImpPlot(rfm)
  model_importance <- importance(rfm)

  #predcit test data
  pp<-predict(rfm, test)
  
  #test data accuracy
  tabb<- table(pp, test$tc_mortality)
  acc<-1-sum(diag(tabb))/sum(tabb) # % missclassified 
  test_accuracy <-(1-acc)*100 #model accuracy
  
  # lists to save model outputs
  # test_accuracy_list <- append(test_accuracy_list, test_accuracy)
  # write.table(model_importance, file="rfm model tc importance.csv", sep=",",col.names=NA, append=T) 
  # write.table(tabb, file = "mcf tc test accuracy table.csv", sep=",",col.names=NA, append=T)
}


```

Model Results
```{r}

# overall average model accuracy -----------------------------------------------------

accuracy_df <- as.data.frame(unlist(test_accuracy_list))
mean_model_accuracy_1 <- accuracy_df %>% 
  summarize(mean_accuracy = mean(unlist(test_accuracy_list)))

#average model importance ---------------------------------------------------------

model_importance_tidy <- read_csv(here("Model Data Sheets and Code", "rfm model tc importance.csv")) %>% 
  clean_names() %>% 
  filter(not_tc != "not_tc") %>% 
  rename (variables = x1) %>% 
  mutate (variables = str_to_lower(variables)) %>% 
  mutate (not_tc = as.numeric(not_tc),
          tc = as.numeric(tc),
          mean_decrease_accuracy = as.numeric(mean_decrease_accuracy),
          mean_decrease_gini = as.numeric(mean_decrease_gini)
          )

# model importance summary statistics
model_importance_summary <- model_importance_tidy %>% 
  group_by(variables) %>% 
  summarize (not_tc_mean = mean(not_tc),
             tc_mean = mean(tc),
             mean_dec_accuracy_mean = mean(mean_decrease_accuracy),
             mean_decrease_gini_mean = mean(mean_decrease_gini)
  )
file.remove(here::here("Model Data Sheets and Code", "rfm model tc importance.csv"))
# average class accuracy ---------------------------------------------------------

not_tc <- read_csv("mcf tc test accuracy table.csv") 
tc <- read_csv("mcf tc test accuracy table.csv")

not_tc_acc <- not_tc %>% 
  select (X1, not_tc) %>% 
  drop_na() %>% 
  filter (X1 == "not_tc") %>% 
  mutate (not_tc = as.numeric(not_tc),
          class_acc = not_tc / (636-nsample)) %>% 
  summarise(mean = mean(class_acc))

tc_acc <- tc %>% 
  select (X1, tc) %>% 
  drop_na() %>% 
  filter (X1 == "tc") %>%
  mutate (tc = as.numeric(tc),
          class_acc = tc / (99-nsample)) %>% 
  summarise(mean = mean(class_acc))

file.remove(here::here("Model Data Sheets and Code", "mcf tc test accuracy table.csv"))

```
 Didnt get to ALE plots. But model importance shows that slope and elevation are highly important for the type converted class. Not surprised because a lot of stands with the highest mortality were at the top of Big pine mountain which was at the highest elevation in our study area. This could indicate that this is trend is specific to this fire situation and does not necessarily predict that stands with high mortality are highly likely to occur in high elevation zones. 

ROC and AUC
```{r}
# code: https://stats.stackexchange.com/questions/188616/how-can-we-calculate-roc-auc-for-classification-algorithm-such-as-random-forest
rfm_roc <-  roc(train$tc_mortality, rfm$votes[,1])
plot(rfm_roc) # plot comes out a little weird
auc(rfm_roc)

```




### Random Forest via Cross Validation

code from:  https://www.youtube.com/watch?v=84JSk36og34

```{r}
# model parameters---------------------------------------------

# set sample size 
nsample2 = 125
```

```{r}

# equal number of observations
  mcf_high <- mcf[which(mcf$mortality_qual == "High"),]
  mcf_low <- mcf[which(mcf$mortality_qual == "Low"),]
  mcf_moderate <- mcf[which(mcf$mortality_qual == "Moderate"),]
  mcf_none <- mcf[which(mcf$mortality_qual == "None"),]
  
  # pull random observations from the high class and separate
  x_high <- sample(1:nrow(mcf_high), nsample2 , replace = FALSE)
  polygon_data_high_train <- mcf_high[x_high,]
  polygon_data_high_test <- mcf_high[-x_high,]
  # pull random observations from the low class and separate
  x_low <- sample(1:nrow(mcf_low), nsample2 , replace = FALSE)
  polygon_data_low_train <- mcf_low[x_low,]
  polygon_data_low_test <- mcf_low[-x_low,]
  # pull random observations from the moderate class and separate
  x_moderate <- sample(1:nrow(mcf_moderate), nsample2 , replace = FALSE)
  polygon_data_moderate_train <- mcf_moderate[x_moderate,]
  polygon_data_moderate_test <- mcf_moderate[-x_moderate,]
  # pull random observations from the none class and separate
  x_none <- sample(1:nrow(mcf_none), nsample2 , replace = FALSE)
  polygon_data_none_train <- mcf_none[x_none,]
  polygon_data_none_test <- mcf_none[-x_none,]
  
  # combine all random observations
  train <- do.call("rbind", list(polygon_data_none_train, polygon_data_low_train, polygon_data_moderate_train, polygon_data_high_train))
  # recombine classes that were not pulled
  test <- do.call("rbind", list(polygon_data_none_test, polygon_data_low_test, polygon_data_moderate_test, polygon_data_high_test))
# make sure y variable is recgonized as a factor
mortality_qual <- as.factor(mcf$mortality_qual)

#add explanatory variables to a single data frame
explantory_variables <- mcf %>% 
  select(rain, tmax,TPI_five,dem_10m_zaca,flow_accumulation,sr,slope,TWI)

# create folds
folds <-  createMultiFolds(mortality_qual, k=10, times=10)

# train Control
control <- trainControl(method = "repeatedcv", number = 10, repeats = 10, index = folds, savePredictions = T)

# cross validation model for random forest
rf.cv <- train(x= explantory_variables, y=mortality_qual, method="rf", tuneLength = 2, ntree=500, trControl=control)

# summary statistics
rf.cv
```

Accuracy pretty much the same without using cross validation. 

### Chris' modified binomial model (original code)


```{r, eval = F, echo = F}
#Prepare data
all_env_variables <- brick(stack(list.files(path = "E:/zaca/environmental_data/all_variables/stack", full.names = TRUE)))
pine <- shapefile("E:/zaca/chris/ground_truth/all_mixed_pine", stringsAsFactors = TRUE)
all_env_variables <- crop(all_env_variables, extent(pine))
environment_extract <- raster::extract(all_env_variables, pine, method = "simple", fun = mean, na.rm = TRUE, sp = TRUE)

env_df <- as.data.frame(environment_extract) %>% 
  filter(prefire != 0) %>% 
  mutate(dem_scaled = dem_10m_zaca/1000, insolation_scaled = insolation_10m_zaca/100000, tmean_scaled = tmean_10m_zaca/10, southness = abs(180 - aspect_10m_zaca), precip_mm = rain_10m_zaca/100) %>% 
  mutate(postfire_success = as.numeric(as.character(car::recode(postfire, "'0' = 0; '1_25' = 125; '25_75' = 500; '75_100' = 875")))) %>% 
  mutate(postfire_failure = 1000 - postfire_success) %>% 
  droplevels(.)

env_df$prefire <- relevel(env_df$prefire, ref = "75_100")

blr <- glm(cbind(postfire_success, postfire_failure) ~ TPI_five_10m_zaca + insolation_scaled + prefire, family = binomial(link = "logit"), data = env_df)

blr

#ifelse(condition, yes, no)
# example ifelse code to get probabilities
class.out <- ifelse(prob < 0.25, "None", 
                    ifelse(prob < 0.5, "Low", 
                           ifelse(...)))

table(class.out, actual.classes)
```

### Chris' modified binomial model (modified by amp): 4 classes

```{r}
# model parameters---------------------------------------------

# set sample size 
nsample4 = 125

# set # of iterations to run 

iteration = 500

# lists to save outputs from for loop
test_accuracy_list <- list()
```


```{r}

# for loop binomial model -----------------------------------------------------------------

for (i in 1:iteration) {
# train and test data 
  mcf_high <- mcf[which(mcf$mortality_qual == "High"),]
  mcf_low <- mcf[which(mcf$mortality_qual == "Low"),]
  mcf_moderate <- mcf[which(mcf$mortality_qual == "Moderate"),]
  mcf_none <- mcf[which(mcf$mortality_qual == "None"),]
  
  # pull random observations from the high class and separate
  x_high <- sample(1:nrow(mcf_high), nsample4 , replace = FALSE)
  polygon_data_high_train <- mcf_high[x_high,]
  polygon_data_high_test <- mcf_high[-x_high,]
  # pull random observations from the low class and separate
  x_low <- sample(1:nrow(mcf_low), nsample4 , replace = FALSE)
  polygon_data_low_train <- mcf_low[x_low,]
  polygon_data_low_test <- mcf_low[-x_low,]
  # pull random observations from the moderate class and separate
  x_moderate <- sample(1:nrow(mcf_moderate), nsample4 , replace = FALSE)
  polygon_data_moderate_train <- mcf_moderate[x_moderate,]
  polygon_data_moderate_test <- mcf_moderate[-x_moderate,]
  # pull random observations from the none class and separate
  x_none <- sample(1:nrow(mcf_none), nsample4 , replace = FALSE)
  polygon_data_none_train <- mcf_none[x_none,]
  polygon_data_none_test <- mcf_none[-x_none,]
  
  # combine all random observations
  train <- do.call("rbind", list(polygon_data_none_train, polygon_data_low_train, polygon_data_moderate_train,  polygon_data_high_train)) 
  # recombine classes that were not pulled
  test <- do.call("rbind", list(polygon_data_none_test, polygon_data_low_test, polygon_data_moderate_test, polygon_data_high_test))
  
#model
blr <- glm(cbind(canopy_loss_success, canopy_loss_failure) ~ dem_10m_zaca + rain + prefire + slope + TPI_five, family = binomial(link = "logit"), data = train)

#model outputs
blr
summary(blr)

#predict 
prob <- predict(blr, newdata = test, type = "response")

# ifelse code to get probabilities
class_out <- ifelse(prob < 0.25, "None", 
                    ifelse(prob < 0.5, "Low", 
                           ifelse(prob < 0.75, " Moderate", 
                                  ifelse(prob <1, "High")))) 

# convert probability outputs from vector to data frame and reorder the mortality classes. Having trouble
model_predictions <- data.frame(class_out)

write.table(model_predictions, "test.csv", sep = ",", col.names = NA, append=F)
model_test <- read_csv(here::here("Model Data Sheets and Code", "test.csv"))
model_test <- model_test %>% 
  mutate(class_out = fct_relevel(class_out, levels = c("None", "Low", "Moderate", "High")))
file.remove(here::here("Model Data Sheets and Code", "test.csv"))

# test data accuracy 
tabb <- table(Predicted = model_test$class_out, Actual = test$canopy_loss_success)
acc<-1-sum(diag(tabb))/sum(tabb) #% missclassified

test_accuracy <-(1-acc)*100 #model accuracy

# lists to save model outputs
test_accuracy_list <- append(test_accuracy_list, test_accuracy)
write.table(tabb, file = "binomial model test accuracy table.csv", sep=",",col.names=NA, append=T)


}


```

- I looked at the PCA to pick variables. It was the easiest way to determine which variables were strongly correlated with other variables

Model Results
```{r}
# model output results ---------------------------------------------------------------------------------------

# average model accuracy
accuracy_df <- as.data.frame(unlist(test_accuracy_list)) %>% 
  summarise(accuracy_mean = mean(unlist(test_accuracy)))

# average class accuracy
high <- read_csv("binomial model test accuracy table.csv") %>%  rename("High" = "750")
moderate <- read_csv("binomial model test accuracy table.csv")  %>%  rename("Moderate" = "500")
low <- read_csv("binomial model test accuracy table.csv")  %>%  rename("Low" = "250")
none <- read_csv("binomial model test accuracy table.csv") %>%  rename("None" = "0")


high_mean_acc <- high %>% 
  select (X1, High) %>% 
  drop_na() %>% 
  filter (X1 == "High") %>% 
  mutate (High = as.numeric(High),
          class_acc = High /33) %>% 
  summarise(mean = (sum(class_acc)/iteration)) 

moderate_mean_acc <- moderate %>% 
  select (X1, Moderate) %>% 
  drop_na() %>% 
  filter (X1 == "Moderate") %>%
  mutate (Moderate = as.numeric(Moderate),
          class_acc = Moderate / 35) %>% 
  summarise(mean = (sum(class_acc)/iteration))

low_mean_acc <- low %>% 
  select (X1, Low) %>% 
  drop_na() %>% 
  filter (X1 == "Low") %>%
  mutate (Low = as.numeric(Low),
          class_acc = Low / 41) %>% 
  summarise(mean = (sum(class_acc)/iteration))

none_mean_acc <- none %>% 
  select (X1, None) %>% 
  drop_na() %>% 
  filter (X1 == "None") %>%
  mutate (None = as.numeric(None),
          class_acc = None / 126) %>% 
  summarise(mean = ((sum(class_acc))/iteration))

file.remove(here::here("Model Data Sheets and Code", "binomial model test accuracy table.csv"))


```


Notes: 

- I changed the response varaible from the difference in pre and post fire canopy cover (in your data its the postfire success/failure, in my data its the change in canopy success/failure), to fire induced canopy loss (i.e. mortality) success/failure

- also split up the data into train and test data with an equal number of observations in each mortality class for the train data. The model kept confusing the mortality with low/none since none had such a higher number of observations

- I don't know why I am having so much trouble with reordering the levels of the test data predictions (after converting the list from vector data to a data frame). I keep getting this error: *Outer Names are only allowed for unnamed scalar atomic* and the fct_relevel won't actually reorder correctly. I tried several different codes (factor, fct_relevel, fct-reorder, etc) to order the class, but the only thing that worked was exporting the data into a csv file then reimporting it back into r, then deleting the file (bc its in a for loop). fct_relevel works now. I think R just doesn't like vector data.  

- An issue I was having was when there are no accurate predictions for a mortality class(ie High), that 0 value doesnt get stored in the output table, so when I tried to get the average accuracy for that mortality class I don't have a complete list of the values. I fixed this by suming the acuracy each iteration that appeared in the data set, then dividing that by the total number of iterations run. works. 

- i dont know why I am having so much trouble with converting the test data predictions into a data frame and then reordering the levels. If I get the data from to work then the factor works but moderate is registered as an NA when it wasnt before. If i use mutate fct_relevel then I get this error: fct_relvel bc keep getting this error: Outer names are only allowed for unnamed scalar atomic inputs (even when used mutate to convert the data from atomic data and is.atomic() did not register the data as atomic; use is.atomic(data_name$column_name) to check if r recognizes the data as atomic. I think this issue is occuring bc the data I'm converting to a data frame was originally vector data). Convertign the class_out to a data frame, exporting it to a csv, and then reimporting it was the only way to reorder the levels without having moderate converted to NA


### Chris' modified binomial model (modified by amp): 3 classes-combine low and none classes

```{r}
# model parameters---------------------------------------------

# set sample size 
nsample5 = 125

#set number of foor loop iterations
iteration1 = 500

# sub-data: combine low and no mortality classes
mcf <- mcf %>%
  mutate(mortality_qual_3 = case_when(
    mortality_qual %in% c("None", "Low") ~ "Low",
    mortality_qual == "Moderate" ~ "Moderate",
    mortality_qual == "High" ~ "High"
  )) %>% 
  mutate(canopy_loss_success_3 = case_when(
    canopy_loss_success %in% c("0", "250") ~ "250",
    canopy_loss_success == "500" ~"500",
    canopy_loss_success == "750" ~ "750"
  )) %>% 
  mutate(canopy_loss_failure_3 = case_when(
    canopy_loss_failure %in% c("1000", "750") ~ "750",
    canopy_loss_failure == "500" ~"500",
    canopy_loss_failure == "250" ~ "250"
  ))

mcf$canopy_loss_success_3 <- factor(mcf$canopy_loss_success_3)
mcf$canopy_loss_failure_3 <- factor(mcf$canopy_loss_failure_3)

# lists to save outputs from for loop

test_accuracy_list <- list()
```


```{r}

# for loop binomial model -----------------------------------------------------------------

for (i in 1:iteration1) {
# train and test data 
  mcf_high <- mcf[which(mcf $ mortality_qual_3 == "High"),]
  mcf_low <- mcf[which(mcf $ mortality_qual_3 == "Low"),]
  mcf_moderate <- mcf[which(mcf $ mortality_qual_3 == "Moderate"),]

  # pull random observations from the high class and separate
  x_high <- sample(1:nrow(mcf_high), nsample5, replace = FALSE)
  polygon_data_high_train <- mcf_high[x_high,]
  polygon_data_high_test <- mcf_high[-x_high,]
  # pull random observations from the low class and separate
  x_low <- sample(1:nrow(mcf_low), nsample5 , replace = FALSE)
  polygon_data_low_train <- mcf_low[x_low,]
  polygon_data_low_test <- mcf_low[-x_low,]
  # pull random observations from the moderate class and separate
  x_moderate <- sample(1:nrow(mcf_moderate), nsample5 , replace = FALSE)
  polygon_data_moderate_train <- mcf_moderate[x_moderate,]
  polygon_data_moderate_test <- mcf_moderate[-x_moderate,]

  
  # combine all random observations
  train <- do.call("rbind", list(polygon_data_low_train, polygon_data_moderate_train,  polygon_data_high_train)) 
  # recombine classes that were not pulled
  test <- do.call("rbind", list(polygon_data_low_test, polygon_data_moderate_test, polygon_data_high_test))
  
#model
blr <- glm(cbind(canopy_loss_success_3, canopy_loss_failure_3) ~ dem_10m_zaca + rain + prefire + slope + TPI_five, family = binomial(link = "logit"), data = train)

#model outputs
blr
summary(blr)

#predict 
prob <- predict(blr, newdata = test, type = "response")

# ifelse code to get probabilities
class_out <- ifelse(prob < 0.25, "Low", 
                     ifelse(prob < 0.75, " Moderate", 
                             ifelse(prob <1, "High"))) 

# convert probability outputs from vector to data frame and reorder the mortality classes. 
model_predictions <- data.frame(class_out)

write.table(model_predictions, "test.csv", sep = ",", col.names = NA, append=F)
model_test <- read_csv(here::here("Model Data Sheets and Code", "test.csv"))
model_test <- model_test %>% 
  mutate(class_out = fct_relevel(class_out, levels = c("Low", "Moderate", "High")))
file.remove(here::here("Model Data Sheets and Code", "test.csv"))

# test data accuracy 
tabb <- table(Predicted = model_test$class_out, Actual = test$canopy_loss_success_3)
acc<-1-sum(diag(tabb))/sum(tabb)
test_accuracy <-(1-acc)*100 #model accuracy

# lists to save model outputs
test_accuracy_list <- append(test_accuracy_list, test_accuracy)
write.table(tabb, file = "binomial model test accuracy table.csv", sep=",",col.names=NA, append=T)

}

```

Model Results
```{r}
# model output results ---------------------------------------------------------------------------------------

# average model accuracy
accuracy_df <- as.data.frame(unlist(test_accuracy_list)) %>% 
  summarise(accuracy_mean = mean(unlist(test_accuracy)))

# average class accuracy
high <- read_csv("binomial model test accuracy table.csv") %>%  rename("High" = "750")
moderate <- read_csv("binomial model test accuracy table.csv")  %>%  rename("Moderate" = "500")
low <- read_csv("binomial model test accuracy table.csv")  %>%  rename("Low" = "250")

high_mean_acc <- high %>% 
  select (X1, High) %>% 
  drop_na() %>% 
  filter (X1 == "High") %>% 
  mutate (High = as.numeric(High),
          class_acc = High / 33) %>% 
  summarise(mean = (sum(class_acc)/iteration))

moderate_mean_acc <- moderate %>% 
  select (X1, Moderate) %>% 
  drop_na() %>% 
  filter (X1 == "Moderate") %>%
  mutate (Moderate = as.numeric(Moderate),
          class_acc = Moderate / 35)%>% 
  summarise(mean = (sum(class_acc)/iteration))

low_mean_acc <- low %>% 
  select (X1, Low) %>% 
  drop_na() %>% 
  filter (X1 == "Low") %>%
  mutate (Low = as.numeric(Low),
          class_acc = Low / 292) %>% 
 summarise(mean = (sum(class_acc)/iteration)) # used this code before to get mean: summarise(mean = mean(class_acc))

file.remove(here::here("Model Data Sheets and Code", "binomial model test accuracy table.csv"))

```






### Future Models

Chose the GFDL-CM3 model which is from the NOAA Geophysical Fluid Dynamics Laboratory. Wasn't sure which one to pick, but NOAA is a reputable organization. 

- LOCA VIR RUN (Scripps): Has monthly data for RCP 4.5 and 8.5 for precipitation and "air temperature"" 
- LOCA Downscaled CMIP5 Climate Projections (Scripps): daily and annual data for RCP 4.5 and 8.5 (no monthly data) for min temp, max temp, precip, solar radiation, relative humidity
- LOCA Derived Products: mixture of things. RCP 4.5 and 8.5. No monthly data for climate variables
- units (for CNRM-CM5, CanESM2): temperature = Kelvin, precipitation = kg/m2/s, solar radiation = W/m2
- unsure about units from HadGEM2, GFDL-CM3, or MIROC5 (may be same as above)

- *** so I started downloading data to start this analysis and then realized the precipitation values are in flux format. Worse, in units of seconds, not days. I may learn how to deal with this type of data for future modelingspring quarter, but not there yet. 



